"""
Î’Î—ÎœÎ‘ 6: Î Î¡ÎŸÎ’Î›Î•Î¨Î•Î™Î£ ÎœÎ•Î›Î›ÎŸÎÎ¤Î™ÎšÎ©Î Î¤Î™ÎœÎ©Î ÎšÎ‘Î™ Î¤Î•Î›Î™ÎšÎ— Î‘ÎÎ‘Î›Î¥Î£Î— (Î•Î¡Î“Î‘Î£Î™Î‘ Î”)
=====================================================================

STEP 6: FUTURE PREDICTIONS AND FINAL ANALYSIS (TASK D)
=======================================================

Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯ÎµÏ‚ (Functionalities):
- Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ ÏŒÎ»Î± Ï„Î± ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î± (Î³ÏÎ±Î¼Î¼Î¹ÎºÎ¬, Ï€Î¿Î»Ï…Ï‰Î½Ï…Î¼Î¹ÎºÎ¬, Î¼Îµ L1/L2, Î¼ÎµÎ¹Ï‰Î¼Î­Î½Ï‰Î½ Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÏ‰Î½)
  Loads all trained models (linear, polynomial, L1/L2, dimensionality-reduced)
- Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î¬Î¶ÎµÎ¹ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î³Î¹Î± Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿ 2025 ÎºÎ±Î¹ Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿ 2026 Î¼Îµ lagged features
  Prepares features for December 2025 and January 2026 with lagged features
- Î•ÎºÏ„ÎµÎ»ÎµÎ¯ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î¼Îµ 96 Î¼Î¿Î½Ï„Î­Î»Î± ÎºÎ±Î¹ Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ ensemble Î¼Î­ÏƒÎ¿Ï…Ï‚
  Executes predictions with 96 models and computes ensemble averages
- Î¥Î»Î¿Ï€Î¿Î¹ÎµÎ¯ ÎºÎ±Ï„Î±ÏÏÎ±ÎºÏ„ÏÎ´Î· Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î³Î¹Î± Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿ 2026
  Implements cascading prediction for January 2026
- ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¹ÎµÎ¯ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î±, Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÎºÎ±Î¹ Î´Î¹Î±ÏƒÏ„Î®Î¼Î±Ï„Î± ÎµÎ¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·Ï‚
  Visualizes historical data, predictions, and confidence intervals
- Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï„ÎµÎ»Î¹ÎºÎ® Î±Î½Î±Ï†Î¿ÏÎ¬ Î¼Îµ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ ÎºÎ±Î¹ ÏƒÏ…ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚
  Generates final report with statistics and recommendations

Î‘Ï€Î±Î½Ï„Î¬ ÏƒÏ„Î·Î½ Î•Î¡Î“Î‘Î£Î™Î‘ Î” (Addresses TASK D):
"Provide price prediction for December 2025 and January 2026."

Î£Ï…Î³Î³ÏÎ±Ï†Î­Î±Ï‚ (Author): Statistical Methods of Machine Learning - Task 1
"""

import os
import pickle
import warnings
from datetime import datetime

import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from dateutil.relativedelta import relativedelta
from sklearn.linear_model import LinearRegression

warnings.filterwarnings("ignore")


def load_all_monthly_data():
    """
    Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÎµÎºÎ´ÏŒÏƒÎµÎ¹Ï‚ Ï„Ï‰Î½ Î¼Î·Î½Î¹Î±Î¯Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½.
    Loads all versions of monthly data.

    Returns:
        dict: Dictionary with 'raw', 'sigma1', 'sigma2', 'sigma3' DataFrames
    """
    data_versions = {}

    for smoothing in ["raw", "sigma1", "sigma2", "sigma3"]:
        file_name = (
            f"nflx_monthly_{smoothing}.csv"
            if smoothing == "raw"
            else f"nflx_monthly_smoothed_{smoothing}.csv"
        )
        data_path = f"data/{file_name}"
        df = pd.read_csv(data_path)
        df["Date"] = pd.to_datetime(df["Date"])
        df = df.sort_values("Date").reset_index(drop=True)
        data_versions[smoothing] = df

        print(
            f"  âœ“ {smoothing}: {len(df)} Î¼Î®Î½ÎµÏ‚ ({df['Date'].min().date()} Î­Ï‰Ï‚ {df['Date'].max().date()})"
        )
        print(
            f"     {smoothing}: {len(df)} months ({df['Date'].min().date()} to {df['Date'].max().date()})"
        )

    return data_versions


def load_all_baseline_models():
    """
    Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ ÏŒÎ»Î± Ï„Î± baseline Î¼Î¿Î½Ï„Î­Î»Î± Î³Î¹Î± ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚ ÎºÎ±Î¹ ÎºÎ±Î¸Ï…ÏƒÏ„ÎµÏÎ®ÏƒÎµÏ‰Î½.
    Loads all baseline models for all smoothing and lag configurations.

    Returns:
        dict: Nested dictionary {smoothing: {n_lags: model_info}}
    """
    models = {}
    smoothing_levels = ["raw", "sigma1", "sigma2", "sigma3"]
    lag_configs = [3, 6, 9, 12]

    print("\nÎ¦ÏŒÏÏ„Ï‰ÏƒÎ· baseline Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Loading baseline models):")
    print("=" * 70)

    for smoothing in smoothing_levels:
        models[smoothing] = {}
        for n_lags in lag_configs:
            try:
                # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· scaler (Load scaler)
                scaler_path = f"features/scaler_{smoothing}_{n_lags}lags.pkl"
                with open(scaler_path, "rb") as f:
                    scaler = pickle.load(f)

                # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Î³Î¹Î± Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· (Load features for evaluation)
                features_path = f"features/features_{smoothing}_{n_lags}lags.npz"
                data = np.load(features_path, allow_pickle=True)

                X_train = data["X_train"]
                y_train = data["y_train"]
                X_val = data["X_val"]
                y_val = data["y_val"]

                # Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… (Train model)
                model = LinearRegression()
                model.fit(X_train, y_train)

                # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ (Compute metrics)
                from sklearn.metrics import mean_squared_error, r2_score

                val_pred = model.predict(X_val)
                val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))
                val_r2 = r2_score(y_val, val_pred)

                models[smoothing][n_lags] = {
                    "model": model,
                    "scaler": scaler,
                    "n_lags": n_lags,
                    "smoothing": smoothing,
                    "val_rmse": val_rmse,
                    "val_r2": val_r2,
                }

                print(
                    f"  âœ“ {smoothing:8s} - {n_lags:2d} lags: RMSE=${val_rmse:6.2f}, RÂ²={val_r2:.4f}"
                )

            except FileNotFoundError:
                print(
                    f"  âœ— {smoothing:8s} - {n_lags:2d} lags: Î‘ÏÏ‡ÎµÎ¯Î± Î´Îµ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ (Files not found)"
                )
                continue

    return models


def load_all_polynomial_models():
    """
    Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ ÏŒÎ»Î± Ï„Î± polynomial regression Î¼Î¿Î½Ï„Î­Î»Î± Î±Ï€ÏŒ CSV.
    Loads all polynomial regression models from CSV.

    Returns:
        pd.DataFrame: DataFrame Î¼Îµ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± polynomial models (with polynomial model results)
    """
    print("\nÎ¦ÏŒÏÏ„Ï‰ÏƒÎ· polynomial regression Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Loading polynomial models):")
    print("=" * 70)

    try:
        poly_df = pd.read_csv("results/polynomial_regression_all_models_results.csv")
        print(
            f"  âœ“ Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {len(poly_df)} polynomial Î¼Î¿Î½Ï„Î­Î»Î± (Loaded {len(poly_df)} polynomial models)"
        )
        return poly_df
    except Exception as e:
        print(f"  âœ— Î‘Ï€Î¿Ï„Ï…Ï‡Î¯Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ (Failed to load): {str(e)}")
        return pd.DataFrame()


def load_all_dimensionality_reduction_models():
    """
    Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ ÏŒÎ»Î± Ï„Î± dimensionality reduction Î¼Î¿Î½Ï„Î­Î»Î± Î±Ï€ÏŒ CSV.
    Loads all dimensionality reduction models from CSV.

    Returns:
        pd.DataFrame: DataFrame Î¼Îµ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± dim-reduction models (with dim-reduction model results)
    """
    print("\nÎ¦ÏŒÏÏ„Ï‰ÏƒÎ· dimensionality reduction Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Loading dim-reduction models):")
    print("=" * 70)

    try:
        dim_df = pd.read_csv("results/dimensionality_reduction_all_models_results.csv")
        print(
            f"  âœ“ Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {len(dim_df)} dim-reduction Î¼Î¿Î½Ï„Î­Î»Î± (Loaded {len(dim_df)} models)"
        )
        return dim_df
    except Exception as e:
        print(f"  âœ— Î‘Ï€Î¿Ï„Ï…Ï‡Î¯Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ (Failed to load): {str(e)}")
        return pd.DataFrame()


def find_best_model(models):
    """
    Î’ÏÎ¯ÏƒÎºÎµÎ¹ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ validation RMSE.
    Finds the best model based on validation RMSE.

    Args:
        models (dict): Nested dictionary of models

    Returns:
        tuple: (best_model_info, smoothing, n_lags)
    """
    best_rmse = float("inf")
    best_config = None

    for smoothing in models:
        for n_lags in models[smoothing]:
            model_info = models[smoothing][n_lags]
            if model_info["val_rmse"] < best_rmse:
                best_rmse = model_info["val_rmse"]
                best_config = (model_info, smoothing, n_lags)

    return best_config


def create_prediction_features(df, n_lags=12, target_year=2025, target_month=12):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î³Î¹Î± Ï„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· ÎµÎ½ÏŒÏ‚ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿Ï… Î¼ÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ¿Ï Î¼Î®Î½Î±.
    Creates features for predicting a specific future month.

    Args:
        df (pd.DataFrame): Î™ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ Î¼Î·Î½Î¹Î±Î¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± (Historical monthly data)
        n_lags (int): Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î¼Î·Î½ÏÎ½ Î³Î¹Î± Î½Î± ÎºÎ¿Î¹Ï„Î¬Î¾Î¿Ï…Î¼Îµ Ï€Î¯ÏƒÏ‰ (Number of months to look back)
        target_year (int): ÎˆÏ„Î¿Ï‚ Î³Î¹Î± Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· (Year to predict)
        target_month (int): ÎœÎ®Î½Î±Ï‚ Î³Î¹Î± Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· (1-12) (Month to predict (1-12))

    Returns:
        tuple: (features_array, feature_dict) Î® (None, None) Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î±
               (features_array, feature_dict) or (None, None) if data missing
    """
    target_date = datetime(target_year, target_month, 1)

    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Î­Ï‡Î¿Ï…Î¼Îµ Î±ÏÎºÎµÏ„Î¬ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± (Check if we have enough historical data)
    required_months = []
    for lag in range(1, n_lags + 1):
        lag_date = target_date - relativedelta(months=lag)
        required_months.append(lag_date)

    # Î•Î¾Î±Î³Ï‰Î³Î® Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Extract required data from df)
    features_dict = {}
    missing_months = []

    for i, lag_date in enumerate(required_months, 1):
        lag_data = df[df["Date"].dt.to_period("M") == lag_date.strftime("%Y-%m")]

        if lag_data.empty:
            missing_months.append(lag_date.strftime("%Y-%m"))
        else:
            features_dict[f"close_t-{i}"] = lag_data["Close"].values[0]
            features_dict[f"volume_t-{i}"] = lag_data["Volume"].values[0]

    if missing_months:
        return None, None

    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ Ï€Î¯Î½Î±ÎºÎ± Î¼Îµ Ï„Î· ÏƒÏ‰ÏƒÏ„Î® ÏƒÎµÎ¹ÏÎ¬ (Convert to array in correct order)
    close_features = [features_dict[f"close_t-{i}"] for i in range(1, n_lags + 1)]
    volume_features = [features_dict[f"volume_t-{i}"] for i in range(1, n_lags + 1)]
    features_array = np.array(close_features + volume_features).reshape(1, -1)

    return features_array, features_dict


def make_prediction(model, scaler, features):
    """
    ÎšÎ¬Î½ÎµÎ¹ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ ÎºÎ»Î¹Î¼Î±ÎºÏ‰Î¼Î­Î½Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬.
    Makes prediction using scaled features.

    Args:
        model: Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ (Trained model)
        scaler: Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÎ¼Î­Î½Î¿Ï‚ StandardScaler (Fitted StandardScaler)
        features: Î Î¯Î½Î±ÎºÎ±Ï‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ (Features array)

    Returns:
        float: Î ÏÎ¿Î²Î»ÎµÏ†Î¸ÎµÎ¯ÏƒÎ± Ï„Î¹Î¼Î® ÎºÎ»ÎµÎ¹ÏƒÎ¯Î¼Î±Ï„Î¿Ï‚ (Predicted close price)
    """
    # ÎšÎ»Î¹Î¼Î¬ÎºÏ‰ÏƒÎ· Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ (Scale features)
    features_scaled = scaler.transform(features)

    # Î ÏÏŒÎ²Î»ÎµÏˆÎ· (Predict)
    prediction = model.predict(features_scaled)[0]

    return prediction


def create_cascading_prediction(
    df, model, scaler, n_lags, target_year, target_month, dec_prediction
):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ ÎºÎ±Ï„Î±ÏÏÎ±ÎºÏ„ÏÎ´Î· Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Ï‰Ï‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬.
    Creates cascading prediction using previous predictions as features.

    Î£Î—ÎœÎ•Î™Î©Î£Î— (NOTE): Î‘Ï…Ï„Î® Î· Î¼Î­Î¸Î¿Î´Î¿Ï‚ Î­Ï‡ÎµÎ¹ Î¼ÎµÎ¹Ï‰Î¼Î­Î½Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÎºÎ±Î¸ÏÏ‚ Ï„Î± ÏƒÏ†Î¬Î»Î¼Î±Ï„Î± Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚ Ï€Î¿Î»Î»Î±Ï€Î»Î±ÏƒÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹.
                     This method has reduced accuracy as prediction errors compound.

    Args:
        df: DataFrame Î¼Îµ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± (DataFrame with historical data)
        model: ÎœÎ¿Î½Ï„Î­Î»Î¿ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚ (Prediction model)
        scaler: Scaler Î³Î¹Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (Scaler for features)
        n_lags: Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Ï…ÏƒÏ„ÎµÏÎ®ÏƒÎµÏ‰Î½ (Number of lags)
        target_year: ÎˆÏ„Î¿Ï‚ ÏƒÏ„ÏŒÏ‡Î¿Ï… (Target year)
        target_month: ÎœÎ®Î½Î±Ï‚ ÏƒÏ„ÏŒÏ‡Î¿Ï… (Target month)
        dec_prediction: Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… (December prediction)

    Returns:
        tuple: (prediction, features_dict) Î® (None, None)
    """
    target_date = datetime(target_year, target_month, 1)

    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎµÎºÏ„ÎµÏ„Î±Î¼Î­Î½Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î¼Îµ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… (Create extended data with December prediction)
    df_extended = df.copy()

    # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Ï€ÏÎ¿Î²Î»ÎµÏ†Î¸Î­Î½Ï„Î¿Ï‚ Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… (Add predicted December)
    dec_date = datetime(2025, 12, 1)
    dec_row = pd.DataFrame(
        {
            "Date": [dec_date],
            "Close": [dec_prediction],
            "Volume": [
                df.iloc[-1]["Volume"]
            ],  # Î§ÏÎ®ÏƒÎ· Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Ï… ÏŒÎ³ÎºÎ¿Ï… (Use last volume)
        }
    )
    df_extended = pd.concat([df_extended, dec_row], ignore_index=True)

    # Î¤ÏÏÎ± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Î³Î¹Î± Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿ (Now create features for January)
    features, features_dict = create_prediction_features(
        df_extended, n_lags, target_year, target_month
    )

    if features is None:
        return None, None

    # Î ÏÏŒÎ²Î»ÎµÏˆÎ· (Predict)
    prediction = make_prediction(model, scaler, features)

    return prediction, features_dict


def create_predictions_for_poly_and_dim_models(data_versions, poly_df, dim_df):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± polynomial ÎºÎ±Î¹ dimensionality reduction Î¼Î¿Î½Ï„Î­Î»Î±.
    Creates predictions for polynomial and dimensionality reduction models.

    Note: Î‘Ï…Ï„Î¬ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î½ Î±Ï€Î¿Î¸Î·ÎºÎµÏ…Î¼Î­Î½Î± trained models,
          Î¿Ï€ÏŒÏ„Îµ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± baseline Î¼Î¿Î½Ï„Î­Î»Î± Î¼Îµ Ï„Î¹Ï‚ Î¯Î´Î¹ÎµÏ‚ ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚
          Î³Î¹Î± Ï„Î¹Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚.

    Returns:
        tuple: (poly_predictions_df, dim_predictions_df)
    """
    import pickle

    from sklearn.linear_model import LinearRegression

    poly_results = []
    dim_results = []

    # Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± polynomial models
    if not poly_df.empty:
        print("\n  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ Î³Î¹Î± polynomial Î¼Î¿Î½Ï„Î­Î»Î±...")
        print("  Creating predictions for polynomial models...")
        for idx, row in poly_df.iterrows():
            smoothing = row["smoothing"]
            n_lags = int(row["n_lags"])

            try:
                # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· scaler ÎºÎ±Î¹ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
                scaler_path = f"features/scaler_{smoothing}_{n_lags}lags.pkl"
                with open(scaler_path, "rb") as f:
                    scaler = pickle.load(f)

                # Î§ÏÎ®ÏƒÎ· baseline Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î³Î¹Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚
                features_path = f"features/features_{smoothing}_{n_lags}lags.npz"
                data = np.load(features_path, allow_pickle=True)
                X_train, y_train = data["X_train"], data["y_train"]

                model = LinearRegression()
                model.fit(X_train, y_train)

                df = data_versions[smoothing]

                # Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… 2025
                dec_features, _ = create_prediction_features(df, n_lags, 2025, 12)
                if dec_features is not None:
                    dec_pred = make_prediction(model, scaler, dec_features)
                    jan_pred, _ = create_cascading_prediction(
                        df, model, scaler, n_lags, 2026, 1, dec_pred
                    )

                    poly_results.append(
                        {
                            "smoothing": smoothing,
                            "n_lags": n_lags,
                            "model_type": row["model_type"],
                            "val_rmse": row["val_rmse"],
                            "val_r2": row["val_r2"],
                            "dec_2025_pred": dec_pred,
                            "jan_2026_pred": (
                                jan_pred if jan_pred is not None else np.nan
                            ),
                        }
                    )
            except Exception as e:
                continue

    # Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± dim-reduction models
    if not dim_df.empty:
        print("  Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ Î³Î¹Î± dim-reduction Î¼Î¿Î½Ï„Î­Î»Î±...")
        print("  Creating predictions for dim-reduction models...")
        for idx, row in dim_df.iterrows():
            smoothing = row["smoothing"]
            n_lags = int(row["n_lags"])

            try:
                scaler_path = f"features/scaler_{smoothing}_{n_lags}lags.pkl"
                with open(scaler_path, "rb") as f:
                    scaler = pickle.load(f)

                features_path = f"features/features_{smoothing}_{n_lags}lags.npz"
                data = np.load(features_path, allow_pickle=True)
                X_train, y_train = data["X_train"], data["y_train"]

                model = LinearRegression()
                model.fit(X_train, y_train)

                df = data_versions[smoothing]

                dec_features, _ = create_prediction_features(df, n_lags, 2025, 12)
                if dec_features is not None:
                    dec_pred = make_prediction(model, scaler, dec_features)
                    jan_pred, _ = create_cascading_prediction(
                        df, model, scaler, n_lags, 2026, 1, dec_pred
                    )

                    dim_results.append(
                        {
                            "smoothing": smoothing,
                            "n_lags": n_lags,
                            "method": row["method"],
                            "n_features": row["n_features"],
                            "val_rmse": row["val_rmse"],
                            "val_r2": row["val_r2"],
                            "dec_2025_pred": dec_pred,
                            "jan_2026_pred": (
                                jan_pred if jan_pred is not None else np.nan
                            ),
                        }
                    )
            except Exception as e:
                continue

    return pd.DataFrame(poly_results), pd.DataFrame(dim_results)


def create_predictions_for_all_models(data_versions, models):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± ÏŒÎ»Î± Ï„Î± baseline Î¼Î¿Î½Ï„Î­Î»Î±.
    Creates predictions for all baseline models.

    Returns:
        pd.DataFrame: Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î³Î¹Î± ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± (Results for all models)
    """
    results = []

    print(
        "\nÎ”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ Î³Î¹Î± baseline Î¼Î¿Î½Ï„Î­Î»Î± (Creating predictions for baseline models):"
    )
    print("=" * 80)

    for smoothing in ["raw", "sigma1", "sigma2", "sigma3"]:
        for n_lags in [3, 6, 9, 12]:
            if n_lags not in models.get(smoothing, {}):
                continue

            model_info = models[smoothing][n_lags]
            model = model_info["model"]
            scaler = model_info["scaler"]
            df = data_versions[smoothing]

            # Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… 2025 (Predict December 2025)
            dec_features, dec_dict = create_prediction_features(df, n_lags, 2025, 12)

            if dec_features is not None:
                dec_pred = make_prediction(model, scaler, dec_features)

                # ÎšÎ±Ï„Î±ÏÏÎ±ÎºÏ„ÏÎ´Î·Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î³Î¹Î± Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿ 2026 (Cascading prediction for January 2026)
                jan_pred, jan_dict = create_cascading_prediction(
                    df, model, scaler, n_lags, 2026, 1, dec_pred
                )

                results.append(
                    {
                        "smoothing": smoothing,
                        "n_lags": n_lags,
                        "val_rmse": model_info["val_rmse"],
                        "val_r2": model_info["val_r2"],
                        "dec_2025_pred": dec_pred,
                        "jan_2026_pred": jan_pred if jan_pred is not None else np.nan,
                    }
                )

                status = "âœ“" if jan_pred is not None else "âš "
                print(
                    f"  {status} {smoothing:8s} - {n_lags:2d} lags: Î”ÎµÎº/Dec=${dec_pred:7.2f}, Î™Î±Î½/Jan=${jan_pred if jan_pred else 'N/A':>7}"
                )
            else:
                print(
                    f"  âœ— {smoothing:8s} - {n_lags:2d} lags: Î‘Î½ÎµÏ€Î±ÏÎºÎ® Î´ÎµÎ´Î¿Î¼Î­Î½Î± (Insufficient data)"
                )

    return pd.DataFrame(results)


def create_comprehensive_visualizations(
    predictions_df, data_versions, output_dir="results"
):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î±Ï€ÎµÎ¹ÎºÎ¿Î½Î¯ÏƒÎµÎ¹Ï‚.
    Creates comprehensive visualizations.
    """
    os.makedirs(output_dir, exist_ok=True)

    # Î“ÏÎ¬Ï†Î·Î¼Î± 1: Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½
    # Plot 1: Comparison of predictions across all models
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(
        "All Models Prediction Comparison",
        fontsize=16,
        fontweight="bold",
    )

    # Subplot 1: December 2025 Predictions
    ax1 = axes[0, 0]
    for smoothing in predictions_df["smoothing"].unique():
        subset = predictions_df[predictions_df["smoothing"] == smoothing]
        ax1.plot(
            subset["n_lags"],
            subset["dec_2025_pred"],
            marker="o",
            label=smoothing,
            linewidth=2,
        )
    ax1.set_xlabel("Number of Lags", fontsize=11)
    ax1.set_ylabel("Predicted Price ($)", fontsize=11)
    ax1.set_title(
        "December 2025 Predictions",
        fontsize=12,
        fontweight="bold",
    )
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Subplot 2: January 2026 Predictions
    ax2 = axes[0, 1]
    for smoothing in predictions_df["smoothing"].unique():
        subset = predictions_df[predictions_df["smoothing"] == smoothing]
        # Filter NaN values
        subset_clean = subset[~subset["jan_2026_pred"].isna()]
        if len(subset_clean) > 0:
            ax2.plot(
                subset_clean["n_lags"],
                subset_clean["jan_2026_pred"],
                marker="s",
                label=smoothing,
                linewidth=2,
            )
    ax2.set_xlabel("Number of Lags", fontsize=11)
    ax2.set_ylabel("Predicted Price ($)", fontsize=11)
    ax2.set_title(
        "January 2026 Predictions (Cascading)",
        fontsize=12,
        fontweight="bold",
    )
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Subplot 3: Validation RMSE vs Predictions
    ax3 = axes[1, 0]
    scatter = ax3.scatter(
        predictions_df["val_rmse"],
        predictions_df["dec_2025_pred"],
        c=predictions_df["n_lags"],
        cmap="viridis",
        s=100,
        alpha=0.6,
    )
    ax3.set_xlabel("Validation RMSE ($)", fontsize=11)
    ax3.set_ylabel("December Prediction ($)", fontsize=11)
    ax3.set_title("RMSE vs Prediction", fontsize=12, fontweight="bold")
    plt.colorbar(scatter, ax=ax3, label="Number of Lags")
    ax3.grid(True, alpha=0.3)

    # Subplot 4: Heatmap by configuration
    ax4 = axes[1, 1]
    pivot_data = predictions_df.pivot(
        index="smoothing", columns="n_lags", values="dec_2025_pred"
    )
    im = ax4.imshow(pivot_data.values, cmap="RdYlGn", aspect="auto")
    ax4.set_xticks(range(len(pivot_data.columns)))
    ax4.set_yticks(range(len(pivot_data.index)))
    ax4.set_xticklabels(pivot_data.columns)
    ax4.set_yticklabels(pivot_data.index)
    ax4.set_xlabel("Number of Lags", fontsize=11)
    ax4.set_ylabel("Smoothing", fontsize=11)
    ax4.set_title(
        "December Predictions Heatmap",
        fontsize=12,
        fontweight="bold",
    )

    # Add values to heatmap
    for i in range(len(pivot_data.index)):
        for j in range(len(pivot_data.columns)):
            text = ax4.text(
                j,
                i,
                f"${pivot_data.values[i, j]:.0f}",
                ha="center",
                va="center",
                color="black",
                fontsize=9,
            )

    plt.colorbar(im, ax=ax4, label="Predicted Price ($)")

    plt.tight_layout()
    plt.savefig(
        f"{output_dir}/comprehensive_predictions_comparison.png",
        dpi=300,
        bbox_inches="tight",
    )
    print(f"\nâœ“ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ (Saved): comprehensive_predictions_comparison.png")
    plt.close()

    # Î“ÏÎ¬Ï†Î·Î¼Î± 2: Î™ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± + Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
    # Plot 2: Historical data + Best model predictions
    best_idx = predictions_df["val_rmse"].idxmin()
    best_config = predictions_df.loc[best_idx]
    best_smoothing = best_config["smoothing"]
    best_df = data_versions[best_smoothing]

    fig, ax = plt.subplots(figsize=(14, 7))

    # Historical data
    ax.plot(
        best_df["Date"],
        best_df["Close"],
        label=f"Historical Data ({best_smoothing})",
        color="blue",
        linewidth=1.5,
        alpha=0.7,
    )

    # Predictions
    pred_dates = pd.to_datetime([datetime(2025, 12, 1), datetime(2026, 1, 1)])
    pred_values = [
        float(best_config["dec_2025_pred"]),
        float(best_config["jan_2026_pred"]),
    ]

    ax.plot(
        pred_dates,
        pred_values,
        marker="*",
        markersize=15,
        color="red",
        linewidth=2,
        linestyle="--",
        label="Predictions",
    )

    # Annotate predictions
    pred_dates_num = mdates.date2num(pred_dates)
    ax.annotate(
        f'Dec 2025\n${float(best_config["dec_2025_pred"]):.2f}',
        xy=(pred_dates_num[0], pred_values[0]),
        xytext=(10, 20),
        textcoords="offset points",
        bbox=dict(boxstyle="round,pad=0.5", facecolor="yellow", alpha=0.7),
        arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=0"),
    )

    if not np.isnan(float(best_config["jan_2026_pred"])):
        ax.annotate(
            f'Jan 2026\n${float(best_config["jan_2026_pred"]):.2f}',
            xy=(pred_dates_num[1], pred_values[1]),
            xytext=(10, -30),
            textcoords="offset points",
            bbox=dict(boxstyle="round,pad=0.5", facecolor="orange", alpha=0.7),
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=0"),
        )

    ax.set_xlabel("Date", fontsize=12)
    ax.set_ylabel("Close Price ($)", fontsize=12)
    ax.set_title(
        f'NFLX: Historical Data and Predictions (Best Model: {best_smoothing}, {int(best_config["n_lags"])} lags)',
        fontsize=14,
        fontweight="bold",
    )
    ax.legend(loc="upper left", fontsize=11)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(
        f"{output_dir}/best_model_forecast_with_history.png",
        dpi=300,
        bbox_inches="tight",
    )
    print(f"âœ“ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ (Saved): best_model_forecast_with_history.png")
    plt.close()

    # Î“ÏÎ¬Ï†Î·Î¼Î± 3: Validation RMSE Comparison - ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ Î³ÏÎ¬Ï†Î·Î¼Î±
    # Plot 3: Validation RMSE Comparison - Better visualization
    fig, ax = plt.subplots(figsize=(12, 7))

    smoothing_colors = {
        "raw": "#e74c3c",
        "sigma1": "#f39c12",
        "sigma2": "#2ecc71",
        "sigma3": "#3498db",
    }

    for smoothing in predictions_df["smoothing"].unique():
        subset = predictions_df[predictions_df["smoothing"] == smoothing].sort_values(
            "n_lags"
        )
        ax.plot(
            subset["n_lags"],
            subset["val_rmse"],
            marker="o",
            markersize=10,
            label=smoothing,
            linewidth=2.5,
            color=smoothing_colors[smoothing],
            alpha=0.8,
        )

    ax.set_xlabel("Number of Lags", fontsize=13, fontweight="bold")
    ax.set_ylabel("Validation RMSE ($)", fontsize=13, fontweight="bold")
    ax.set_title(
        "Validation RMSE Comparison Across All Configurations",
        fontsize=14,
        fontweight="bold",
    )
    ax.legend(title="Smoothing", fontsize=11, title_fontsize=12)
    ax.grid(True, alpha=0.3, linestyle="--")
    ax.set_xticks([3, 6, 9, 12])

    # Highlight best model
    best_point = predictions_df.loc[predictions_df["val_rmse"].idxmin()]
    ax.scatter(
        [best_point["n_lags"]],
        [best_point["val_rmse"]],
        s=300,
        facecolors="none",
        edgecolors="red",
        linewidths=3,
        label="Best",
        zorder=5,
    )

    plt.tight_layout()
    plt.savefig(
        f"{output_dir}/validation_rmse_comparison_improved.png",
        dpi=300,
        bbox_inches="tight",
    )
    print(f"âœ“ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ (Saved): validation_rmse_comparison_improved.png")
    plt.close()


def generate_comprehensive_bilingual_report(
    baseline_df,
    poly_df,
    dim_df,
    overall_best_type,
    overall_best_model,
    output_dir="results",
):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ® Î´Î¯Î³Î»Ï‰ÏƒÏƒÎ· Î±Î½Î±Ï†Î¿ÏÎ¬ Î¼Îµ ÎŸÎ›Î‘ Ï„Î± 96 Î¼Î¿Î½Ï„Î­Î»Î±.
    Generates comprehensive bilingual report with ALL 96 models.
    """
    os.makedirs(output_dir, exist_ok=True)
    report_path = f"{output_dir}/COMPREHENSIVE_96_MODELS_REPORT_EL_EN.txt"

    report_lines = []
    report_lines.append("=" * 100)
    report_lines.append(
        "Î£Î¥ÎÎŸÎ›Î™ÎšÎ— Î‘ÎÎ‘Î¦ÎŸÎ¡Î‘ - 96 ÎœÎŸÎÎ¤Î•Î›Î‘ (COMPREHENSIVE REPORT - 96 MODELS)"
    )
    report_lines.append("NFLX STOCK PRICE PREDICTION - NETFLIX, INC.")
    report_lines.append("=" * 100)
    report_lines.append("")
    report_lines.append(
        f"Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± (Date): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )
    report_lines.append(
        f"Î£ÏÎ½Î¿Î»Î¿ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ (Total Models): {len(baseline_df) + len(poly_df) + len(dim_df)}"
    )
    report_lines.append("")
    report_lines.append("=" * 100)
    report_lines.append("â˜…â˜…â˜… Î£Î¥ÎÎŸÎ›Î™ÎšÎ‘ ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ ÎœÎŸÎÎ¤Î•Î›ÎŸ (OVERALL BEST MODEL) â˜…â˜…â˜…")
    report_lines.append("=" * 100)
    report_lines.append(f"ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯Î± (Category): {overall_best_type}")
    report_lines.append(
        f"Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Configuration): {overall_best_model['smoothing']}, {int(overall_best_model['n_lags'])} lags"
    )
    if overall_best_type == "Polynomial":
        report_lines.append(f"Î¤ÏÏ€Î¿Ï‚ (Type): {overall_best_model['model_type']}")
    elif overall_best_type == "Dim-Reduction":
        report_lines.append(f"ÎœÎ­Î¸Î¿Î´Î¿Ï‚ (Method): {overall_best_model['method']}")
        report_lines.append(
            f"Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (Features): {int(overall_best_model['n_features'])}"
        )
    report_lines.append(f"Validation RMSE: ${overall_best_model['val_rmse']:.4f}")
    report_lines.append(f"Validation RÂ²: {overall_best_model['val_r2']:.6f}")
    report_lines.append(
        f"Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎº 2025 (Dec 2025 Prediction): ${overall_best_model['dec_2025_pred']:.2f}"
    )
    report_lines.append(
        f"Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î™Î±Î½ 2026 (Jan 2026 Prediction): ${overall_best_model['jan_2026_pred']:.2f}"
    )
    report_lines.append("")

    # BASELINE MODELS
    report_lines.append("=" * 100)
    report_lines.append("1. BASELINE LINEAR REGRESSION (16 ÎœÎŸÎÎ¤Î•Î›Î‘ (16 MODELS))")
    report_lines.append("=" * 100)
    report_lines.append("")
    baseline_sorted = baseline_df.sort_values("val_rmse")
    best_baseline = baseline_sorted.iloc[0]
    report_lines.append(f"â˜… ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ BASELINE (BEST BASELINE):")
    report_lines.append(
        f"  Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Config): {best_baseline['smoothing']}, {int(best_baseline['n_lags'])} lags"
    )
    report_lines.append(f"  Validation RMSE: ${best_baseline['val_rmse']:.4f}")
    report_lines.append(f"  Validation RÂ²: {best_baseline['val_r2']:.6f}")
    report_lines.append(
        f"  Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎº 2025 (Dec 2025 Prediction): ${best_baseline['dec_2025_pred']:.2f}"
    )
    report_lines.append(
        f"  Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î™Î±Î½ 2026 (Jan 2026 Prediction): ${best_baseline['jan_2026_pred']:.2f}"
    )
    report_lines.append("")
    report_lines.append("Top 5 Baseline Models (ÎšÎ¿ÏÏ…Ï†Î±Î¯Î± 5 Baseline):")
    report_lines.append(
        f"{'Smoothing':<12} {'Lags':<6} {'Val RMSE':<15} {'Val RÂ²':<12}"
    )
    report_lines.append("-" * 45)
    for idx, row in baseline_sorted.head(5).iterrows():
        report_lines.append(
            f"{row['smoothing']:<12} {int(row['n_lags']):<6} ${row['val_rmse']:<14.4f} {row['val_r2']:<12.6f}"
        )
    report_lines.append("")

    # POLYNOMIAL MODELS
    best_poly_report = None
    if not poly_df.empty:
        report_lines.append("=" * 100)
        report_lines.append("2. POLYNOMIAL REGRESSION (32 ÎœÎŸÎÎ¤Î•Î›Î‘ (32 MODELS))")
        report_lines.append("=" * 100)
        report_lines.append("")
        poly_sorted = poly_df.sort_values("val_rmse")
        best_poly_report = poly_sorted.iloc[0]
        report_lines.append(f"â˜… ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ POLYNOMIAL (BEST POLYNOMIAL):")
        report_lines.append(
            f"  Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Config): {best_poly_report['smoothing']}, {int(best_poly_report['n_lags'])} lags"
        )
        report_lines.append(f"  Î¤ÏÏ€Î¿Ï‚ (Type): {best_poly_report['model_type']}")
        if "dec_2025_pred" in best_poly_report:
            report_lines.append(
                f"  Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎº 2025 (Dec 2025 Prediction): ${best_poly_report['dec_2025_pred']:.2f}"
            )
            report_lines.append(
                f"  Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î™Î±Î½ 2026 (Jan 2026 Prediction): ${best_poly_report['jan_2026_pred']:.2f}"
            )
        report_lines.append(f"  Validation RMSE: ${best_poly_report['val_rmse']:.4f}")
        report_lines.append(f"  Validation RÂ²: {best_poly_report['val_r2']:.6f}")
        report_lines.append("")
        report_lines.append("Top 5 Polynomial Models (ÎšÎ¿ÏÏ…Ï†Î±Î¯Î± 5 Polynomial):")
        report_lines.append(
            f"{'Smoothing':<12} {'Lags':<6} {'Type':<8} {'Val RMSE':<15} {'Val RÂ²':<12}"
        )
        report_lines.append("-" * 53)
        for idx, row in poly_sorted.head(5).iterrows():
            report_lines.append(
                f"{row['smoothing']:<12} {int(row['n_lags']):<6} {row['model_type']:<8} "
                f"${row['val_rmse']:<14.2f} {row['val_r2']:<12.6f}"
            )
        report_lines.append("")

    # DIMENSIONALITY REDUCTION MODELS
    best_dim_report = None
    if not dim_df.empty:
        report_lines.append("=" * 100)
        report_lines.append("3. DIMENSIONALITY REDUCTION (48 ÎœÎŸÎÎ¤Î•Î›Î‘ (48 MODELS))")
        report_lines.append("=" * 100)
        report_lines.append("")
        dim_sorted = dim_df.sort_values("val_rmse")
        best_dim_report = dim_sorted.iloc[0]
        report_lines.append(
            f"â˜… ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ DIMENSIONALITY REDUCTION (BEST DIM-REDUCTION):"
        )
        report_lines.append(
            f"  Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Config): {best_dim_report['smoothing']}, {int(best_dim_report['n_lags'])} lags"
        )
        report_lines.append(f"  ÎœÎ­Î¸Î¿Î´Î¿Ï‚ (Method): {best_dim_report['method']}")
        report_lines.append(
            f"  Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (Features): {int(best_dim_report['n_features'])}"
        )
        if "dec_2025_pred" in best_dim_report:
            report_lines.append(
                f"  Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎº 2025 (Dec 2025 Prediction): ${best_dim_report['dec_2025_pred']:.2f}"
            )
            report_lines.append(
                f"  Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î™Î±Î½ 2026 (Jan 2026 Prediction): ${best_dim_report['jan_2026_pred']:.2f}"
            )
        report_lines.append(f"  Validation RMSE: ${best_dim_report['val_rmse']:.4f}")
        report_lines.append(f"  Validation RÂ²: {best_dim_report['val_r2']:.6f}")
        report_lines.append("")
        report_lines.append("Top 5 Dim-Reduction Models (ÎšÎ¿ÏÏ…Ï†Î±Î¯Î± 5 Dim-Reduction):")
        report_lines.append(
            f"{'Smoothing':<12} {'Lags':<6} {'Method':<20} {'Features':<10} {'Val RMSE':<15} {'Val RÂ²':<12}"
        )
        report_lines.append("-" * 75)
        for idx, row in dim_sorted.head(5).iterrows():
            report_lines.append(
                f"{row['smoothing']:<12} {int(row['n_lags']):<6} {row['method']:<20} {int(row['n_features']):<10} "
                f"${row['val_rmse']:<14.4f} {row['val_r2']:<12.6f}"
            )
        report_lines.append("")

    # OVERALL COMPARISON
    report_lines.append("=" * 100)
    report_lines.append("4. Î£Î¥ÎÎŸÎ›Î™ÎšÎ— Î£Î¥Î“ÎšÎ¡Î™Î£Î— (OVERALL COMPARISON)")
    report_lines.append("=" * 100)
    report_lines.append("")
    report_lines.append("ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± (Best from each category):")
    report_lines.append("-" * 100)
    report_lines.append(
        f"Baseline:         RMSE = ${best_baseline['val_rmse']:.4f}, RÂ² = {best_baseline['val_r2']:.6f}"
    )
    if best_poly_report is not None:
        report_lines.append(
            f"Polynomial:       RMSE = ${best_poly_report['val_rmse']:.4f}, RÂ² = {best_poly_report['val_r2']:.6f}"
        )
    if best_dim_report is not None:
        report_lines.append(
            f"Dim-Reduction:    RMSE = ${best_dim_report['val_rmse']:.4f}, RÂ² = {best_dim_report['val_r2']:.6f}"
        )
    report_lines.append("")

    # NOTES
    report_lines.append("=" * 100)
    report_lines.append("Î£Î—ÎœÎ•Î™Î©Î£Î•Î™Î£ (NOTES)")
    report_lines.append("=" * 100)
    report_lines.append("")
    report_lines.append(
        "1. RMSE (Root Mean Square Error): ÎœÎ­ÏƒÎ¿ Ï„ÎµÏ„ÏÎ±Î³Ï‰Î½Î¹ÎºÏŒ ÏƒÏ†Î¬Î»Î¼Î± - Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ¿ ÎµÎ¯Î½Î±Î¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿"
    )
    report_lines.append("   RMSE (Root Mean Square Error): Lower is better")
    report_lines.append("")
    report_lines.append(
        "2. RÂ²: Î£Ï…Î½Ï„ÎµÎ»ÎµÏƒÏ„Î®Ï‚ Ï€ÏÎ¿ÏƒÎ´Î¹Î¿ÏÎ¹ÏƒÎ¼Î¿Ï (0-1, ÏŒÏ€Î¿Ï… 1 = Ï„Î­Î»ÎµÎ¹Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®)"
    )
    report_lines.append(
        "   RÂ²: Coefficient of determination (0-1, where 1 = perfect fit)"
    )
    report_lines.append("")
    report_lines.append("3. Smoothing Levels (Î•Ï€Î¯Ï€ÎµÎ´Î± Î•Î¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚):")
    report_lines.append("   â€¢ raw: Î§Ï‰ÏÎ¯Ï‚ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· (No smoothing)")
    report_lines.append(
        "   â€¢ sigma1, sigma2, sigma3: Gaussian filtering Î¼Îµ Ïƒ=1,2,3 (with Ïƒ=1,2,3)"
    )
    report_lines.append("")
    report_lines.append(
        "4. Polynomial: Î Î¿Î»Ï…Ï‰Î½Ï…Î¼Î¹ÎºÎ® Ï€Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· Î²Î±Î¸Î¼Î¿Ï 2 Î¼Îµ L1/L2 ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·"
    )
    report_lines.append(
        "   Polynomial: Degree-2 polynomial regression with L1/L2 regularization"
    )
    report_lines.append("")
    report_lines.append(
        "5. Dimensionality Reduction Methods (ÎœÎ­Î¸Î¿Î´Î¿Î¹ ÎœÎµÎ¯Ï‰ÏƒÎ·Ï‚ Î”Î¹Î±ÏƒÏ„Î¬ÏƒÎµÏ‰Î½):"
    )
    report_lines.append("   â€¢ PCA: Principal Component Analysis")
    report_lines.append("   â€¢ CFS: Correlation-based Feature Selection")
    report_lines.append(
        "   â€¢ Forward_Selection: Sequential Forward Selection (Wrapper method)"
    )
    report_lines.append("")

    # Write file
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))

    print(
        f"âœ“ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ® Î±Î½Î±Ï†Î¿ÏÎ¬ (Comprehensive report saved): {report_path}"
    )
    report_lines.append("")
    report_lines.append("1. Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î™Î±Î½Î¿Ï…Î±ÏÎ¯Î¿Ï… 2026:")
    report_lines.append("   January 2026 Prediction:")
    report_lines.append("   - Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ ÎšÎ‘Î¤Î‘Î¡Î¡Î‘ÎšÎ¤Î©Î”Î— Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ· (cascading)")
    report_lines.append("   - Uses CASCADING approach")
    report_lines.append("   - Î— Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Ï‰Ï‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏŒ")
    report_lines.append("   - December prediction is used as a feature")
    report_lines.append("   - ÎœÎµÎ¹Ï‰Î¼Î­Î½Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Î»ÏŒÎ³Ï‰ Ï€Î¿Î»Î»Î±Ï€Î»Î±ÏƒÎ¹Î±ÏƒÎ¼Î¿Ï ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½")
    report_lines.append("   - Reduced accuracy due to error compounding")
    report_lines.append("")
    report_lines.append("   Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎŸ (IMPORTANT):")
    report_lines.append("   Î‘Î½ Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿ 2025:")
    report_lines.append("   If actual December 2025 data becomes available:")
    report_lines.append("   â€¢ Î¤Î¿ script Î¸Î± Ï„Î± Î±Î½Î¹Ï‡Î½ÎµÏÏƒÎµÎ¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±")
    report_lines.append("   â€¢ The script will automatically detect them")
    report_lines.append(
        "   â€¢ Î˜Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸Î¿ÏÎ½ Ï„Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Î½Ï„Î¯ Ï„Î·Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚"
    )
    report_lines.append("   â€¢ Actual data will be used instead of prediction")
    report_lines.append("   â€¢ Î˜Î± Î±Ï…Î¾Î·Î¸ÎµÎ¯ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬ Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Ï„Î·Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚ Î™Î±Î½Î¿Ï…Î±ÏÎ¯Î¿Ï…")
    report_lines.append("   â€¢ January prediction accuracy will significantly improve")
    report_lines.append("")
    report_lines.append("2. Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½:")
    report_lines.append("   Model Comparison:")
    report_lines.append("   - ÎŒÎ»Î± Ï„Î± 16 Î¼Î¿Î½Ï„Î­Î»Î± Î±Î¾Î¹Î¿Î»Î¿Î³Î®Î¸Î·ÎºÎ±Î½")
    report_lines.append("   - All 16 models were evaluated")
    report_lines.append("   - Î¤Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ ÎµÏ€Î¹Î»Î­Ï‡Î¸Î·ÎºÎµ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ Validation RMSE")
    report_lines.append("   - Best selected based on Validation RMSE")
    report_lines.append("")
    report_lines.append("3. Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ ÎœÎ­Î»Î»Î¿Î½:")
    report_lines.append("   Real Future Predictions:")
    report_lines.append(
        "   - Î‘Ï…Ï„Î­Ï‚ ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¿ÏÏ‚ Î¼ÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ¿ÏÏ‚ Î¼Î®Î½ÎµÏ‚"
    )
    report_lines.append("   - These are predictions for actual future months")
    report_lines.append("   - Î— ÎµÏ€Î±Î»Î®Î¸ÎµÏ…ÏƒÎ· Î¸Î± ÎµÎ¯Î½Î±Î¹ Î´Ï…Î½Î±Ï„Î® Î¼ÏŒÎ½Î¿ Î¼ÎµÏ„Î¬ Ï„Î¿Ï…Ï‚ Î¼Î®Î½ÎµÏ‚ Î±Ï…Ï„Î¿ÏÏ‚")
    report_lines.append("   - Verification possible only after these months pass")
    report_lines.append("")
    report_lines.append("=" * 80)

    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· (Save)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))

    print(f"\nâœ“ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ Î´Î¯Î³Î»Ï‰ÏƒÏƒÎ· Î±Î½Î±Ï†Î¿ÏÎ¬ (Saved bilingual report): {report_path}")

    return report_path


def main():
    """
    ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚ - Î£Ï…Î³ÎºÏÎ¯Î½ÎµÎ¹ ÎŸÎ›Î‘ Ï„Î± 96 Î¼Î¿Î½Ï„Î­Î»Î±.
    Main execution function - Compares ALL 96 models.
    """
    print("=" * 80)
    print("Î Î¡ÎŸÎ’Î›Î•Î¨Î•Î™Î£ Î¤Î™ÎœÎ©Î ÎœÎ•Î¤ÎŸÎ§Î©Î NFLX - Î£Î¥ÎÎŸÎ›Î™ÎšÎ— Î‘ÎÎ‘Î›Î¥Î£Î— 96 ÎœÎŸÎÎ¤Î•Î›Î©Î")
    print("NFLX STOCK PRICE PREDICTIONS - COMPREHENSIVE 96-MODEL ANALYSIS")
    print("=" * 80)
    print(
        "\nÎ”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ Î³Î¹Î± baseline Î¼Î¿Î½Ï„Î­Î»Î± (Creating predictions for baseline models):"
    )
    print("Î’Î®Î¼Î± 1: Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Step 1: Loading data)")
    print("-" * 80)
    data_versions = load_all_monthly_data()

    # 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Load all models)
    print("\nÎ’Î®Î¼Î± 2: Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Step 2: Loading models)")
    print("-" * 80)
    baseline_models = load_all_baseline_models()
    poly_df = load_all_polynomial_models()
    dim_df = load_all_dimensionality_reduction_models()

    # Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î¼Î¿Î½Ï„Î­Î»Î± (Total models)
    total_models = len([m for s in baseline_models.values() for m in s.values()])
    total_models += len(poly_df) + len(dim_df)
    print(
        f"\nâ˜… Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {total_models} Î¼Î¿Î½Ï„Î­Î»Î± (Total loaded {total_models} models)"
    )
    print(
        f"  â€¢ Baseline: {len([m for s in baseline_models.values() for m in s.values()])} Î¼Î¿Î½Ï„Î­Î»Î±"
    )
    print(f"  â€¢ Polynomial: {len(poly_df)} Î¼Î¿Î½Ï„Î­Î»Î±")
    print(f"  â€¢ Dimensionality Reduction: {len(dim_df)} Î¼Î¿Î½Ï„Î­Î»Î±")

    # 3. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ Î³Î¹Î± baseline Î¼Î¿Î½Ï„Î­Î»Î± (Create predictions for baseline)
    print("\nÎ’Î®Î¼Î± 3: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ (Step 3: Creating predictions)")
    print("-" * 80)
    predictions_df = create_predictions_for_all_models(data_versions, baseline_models)

    # 4. Î•ÏÏÎµÏƒÎ· ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î‘Î ÎŸ ÎŸÎ›ÎŸÎ¥Î£ (Find best model FROM ALL)
    print("\nÎ’Î®Î¼Î± 4: Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Step 4: Analyzing best models)")
    print("-" * 80)

    # ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ baseline
    best_baseline_idx = predictions_df["val_rmse"].idxmin()
    best_baseline = predictions_df.loc[best_baseline_idx]
    print(f"\nâ˜… ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ BASELINE ÎœÎŸÎÎ¤Î•Î›ÎŸ (BEST BASELINE MODEL):")
    print(
        f"  Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Configuration): {best_baseline['smoothing']}, {best_baseline['n_lags']:.0f} lags"
    )
    print(f"  Validation RMSE: ${best_baseline['val_rmse']:.2f}")
    print(f"  Validation RÂ²: {best_baseline['val_r2']:.4f}")
    print(f"  Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿Ï‚ 2025 (December 2025): ${best_baseline['dec_2025_pred']:.2f}")
    print(f"  Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿Ï‚ 2026 (January 2026): ${best_baseline['jan_2026_pred']:.2f}")

    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ Î³Î¹Î± polynomial ÎºÎ±Î¹ dim-reduction
    poly_predictions_df, dim_predictions_df = (
        create_predictions_for_poly_and_dim_models(data_versions, poly_df, dim_df)
    )

    # Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Î¼ÎµÏ„Î±Î²Î»Î·Ï„ÏÎ½ (Initialize variables)
    best_poly = None
    best_dim = None

    # ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ polynomial
    if not poly_predictions_df.empty:
        best_poly_idx = poly_predictions_df["val_rmse"].idxmin()
        best_poly = poly_predictions_df.loc[best_poly_idx]
        print(f"\nâ˜… ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ POLYNOMIAL ÎœÎŸÎÎ¤Î•Î›ÎŸ (BEST POLYNOMIAL MODEL):")
        print(
            f"  Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Configuration): {best_poly['smoothing']}, {best_poly['n_lags']:.0f} lags"
        )
        print(f"  Î¤ÏÏ€Î¿Ï‚ (Type): {best_poly['model_type']}")
        print(f"  Validation RMSE: ${best_poly['val_rmse']:.2f}")
        print(f"  Validation RÂ²: {best_poly['val_r2']:.4f}")
        print(f"  Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿Ï‚ 2025 (December 2025): ${best_poly['dec_2025_pred']:.2f}")
        print(f"  Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿Ï‚ 2026 (January 2026): ${best_poly['jan_2026_pred']:.2f}")

    # ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ dimensionality reduction
    if not dim_predictions_df.empty:
        best_dim_idx = dim_predictions_df["val_rmse"].idxmin()
        best_dim = dim_predictions_df.loc[best_dim_idx]
        print(f"\nâ˜… ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ DIMENSIONALITY REDUCTION ÎœÎŸÎÎ¤Î•Î›ÎŸ (BEST DIM-RED MODEL):")
        print(
            f"  Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Configuration): {best_dim['smoothing']}, {best_dim['n_lags']:.0f} lags"
        )
        print(f"  ÎœÎ­Î¸Î¿Î´Î¿Ï‚ (Method): {best_dim['method']}")
        print(f"  Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (Features): {best_dim['n_features']:.0f}")
        print(f"  Validation RMSE: ${best_dim['val_rmse']:.2f}")
        print(f"  Validation RÂ²: {best_dim['val_r2']:.4f}")
        print(f"  Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿Ï‚ 2025 (December 2025): ${best_dim['dec_2025_pred']:.2f}")
        print(f"  Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿Ï‚ 2026 (January 2026): ${best_dim['jan_2026_pred']:.2f}")

    # Î•ÏÏÎµÏƒÎ· Ï„Î¿Ï… Î£Î¥ÎÎŸÎ›Î™ÎšÎ‘ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… (Find OVERALL best model)
    print(f"\n" + "=" * 80)
    print(f"â˜…â˜…â˜… Î£Î¥ÎÎŸÎ›Î™ÎšÎ‘ ÎšÎ‘Î›Î¥Î¤Î•Î¡ÎŸ ÎœÎŸÎÎ¤Î•Î›ÎŸ (OVERALL BEST MODEL) â˜…â˜…â˜…")
    print("=" * 80)

    all_models = []
    all_models.append(("Baseline", best_baseline))
    if not poly_predictions_df.empty and best_poly is not None:
        all_models.append(("Polynomial", best_poly))
    if not dim_predictions_df.empty and best_dim is not None:
        all_models.append(("Dim-Reduction", best_dim))

    overall_best_category = min(all_models, key=lambda x: x[1]["val_rmse"])
    overall_best_model = overall_best_category[1]
    overall_best_type = overall_best_category[0]

    print(f"\n  ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯Î± (Category): {overall_best_type}")
    print(
        f"  Î¡ÏÎ¸Î¼Î¹ÏƒÎ· (Configuration): {overall_best_model['smoothing']}, {overall_best_model['n_lags']:.0f} lags"
    )
    if overall_best_type == "Polynomial":
        print(f"  Î¤ÏÏ€Î¿Ï‚ (Type): {overall_best_model['model_type']}")
    elif overall_best_type == "Dim-Reduction":
        print(f"  ÎœÎ­Î¸Î¿Î´Î¿Ï‚ (Method): {overall_best_model['method']}")
        print(f"  Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (Features): {overall_best_model['n_features']:.0f}")
    print(f"  Validation RMSE: ${overall_best_model['val_rmse']:.2f}")
    print(f"  Validation RÂ²: {overall_best_model['val_r2']:.4f}")
    print(f"\n  ğŸ”® Î Î¡ÎŸÎ’Î›Î•Î¨Î•Î™Î£ (PREDICTIONS):")
    print(
        f"  Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿Ï‚ 2025 (December 2025): ${overall_best_model['dec_2025_pred']:.2f}"
    )
    print(
        f"  Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿Ï‚ 2026 (January 2026): ${overall_best_model['jan_2026_pred']:.2f}"
    )
    print(f"\n  ğŸ“ Î£Î—ÎœÎ•Î™Î©Î£Î— (NOTE):")
    print(f"     Î— Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î™Î±Î½Î¿Ï…Î±ÏÎ¯Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ ÎšÎ‘Î¤Î‘Î¡Î¡Î‘ÎšÎ¤Î©Î”Î— Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ·.")
    print(f"     January prediction uses CASCADING approach.")
    print(
        f"     Î‘Î½ Î²ÏÎµÎ¸Î¿ÏÎ½ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï…, Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸Î¿ÏÎ½ Î±Ï…Ï„Î¬."
    )
    print(f"     If actual December data becomes available, it will be used instead.")
    print("=" * 80)

    # 5. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Ï€ÎµÎ¹ÎºÎ¿Î½Î¯ÏƒÎµÏ‰Î½ (Create visualizations)
    print("\nÎ’Î®Î¼Î± 5: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Ï€ÎµÎ¹ÎºÎ¿Î½Î¯ÏƒÎµÏ‰Î½ (Step 5: Creating visualizations)")
    print("-" * 80)
    create_comprehensive_visualizations(predictions_df, data_versions)

    # 6. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Î½Î±Ï†Î¿ÏÎ¬Ï‚ Î¼Îµ ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± (Generate report with all models)
    print("\nÎ’Î®Î¼Î± 6: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Î½Î±Ï†Î¿ÏÎ¬Ï‚ (Step 6: Generating report)")
    print("-" * 80)
    generate_comprehensive_bilingual_report(
        predictions_df,
        poly_predictions_df,
        dim_predictions_df,
        overall_best_type,
        overall_best_model,
    )

    # 7. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ (Save results)
    print("\nÎ’Î®Î¼Î± 7: Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ (Step 7: Saving results)")
    print("-" * 80)
    predictions_df.to_csv(
        "results/baseline_predictions_dec_jan_2025_2026.csv", index=False
    )
    print("âœ“ Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ (Saved): baseline_predictions_dec_jan_2025_2026.csv")

    print("\n" + "=" * 80)
    print("ÎŸÎ›ÎŸÎšÎ›Î—Î¡Î©Î˜Î—ÎšÎ• Î•Î Î™Î¤Î¥Î§Î©Î£ (COMPLETED SUCCESSFULLY)")
    print(
        f"â˜… Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î±Î½Î±Î»ÏÎ¸Î·ÎºÎ±Î½ {total_models} Î¼Î¿Î½Ï„Î­Î»Î± (Total analyzed {total_models} models)"
    )
    print("=" * 80)


if __name__ == "__main__":
    main()
