# NFLX: Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î¤Î¹Î¼ÏÎ½ ÎœÎµÏ„Î¿Ï‡ÏÎ½ Î¼Îµ Î“ÏÎ±Î¼Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ·

# NFLX: Stock Price Prediction with Linear Regression

**Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ ÎœÎ­Î¸Î¿Î´Î¿Î¹ ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ®Ï‚ ÎœÎ¬Î¸Î·ÏƒÎ·Ï‚ - Î•ÏÎ³Î±ÏƒÎ¯Î± 1**  
**Statistical Methods of Machine Learning - Task 1**

---

## ğŸ“Š Î•Ï€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ· ÎˆÏÎ³Î¿Ï… / Project Overview

### Î•Î»Î»Î·Î½Î¹ÎºÎ¬

Î‘Ï…Ï„ÏŒ Ï„Î¿ Î­ÏÎ³Î¿ Ï…Î»Î¿Ï€Î¿Î¹ÎµÎ¯ Î­Î½Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î¿ ÏƒÏÏƒÏ„Î·Î¼Î± Î¼Î·Ï‡Î±Î½Î¹ÎºÎ®Ï‚ Î¼Î¬Î¸Î·ÏƒÎ·Ï‚ Î³Î¹Î± Ï„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Ï„Î¹Î¼ÏÎ½ Î¼ÎµÏ„Î¿Ï‡ÏÎ½ Ï„Î·Ï‚ Netflix (NFLX) Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Î³ÏÎ±Î¼Î¼Î¹ÎºÎ® Ï€Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· ÎºÎ±Î¹ Ï€ÏÎ¿Î·Î³Î¼Î­Î½ÎµÏ‚ Ï„ÎµÏ‡Î½Î¹ÎºÎ­Ï‚. Î¤Î¿ Î­ÏÎ³Î¿ Î±Î½Ï„Î¹Î¼ÎµÏ„Ï‰Ï€Î¯Î¶ÎµÎ¹ Ï„Î­ÏƒÏƒÎµÏÎ¹Ï‚ Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚:

- **Î•ÏÎ³Î±ÏƒÎ¯Î± Î‘**: Baseline Î“ÏÎ±Î¼Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· Î¼Îµ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚
- **Î•ÏÎ³Î±ÏƒÎ¯Î± Î’**: Î Î¿Î»Ï…Ï‰Î½Ï…Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· Î¼Îµ ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· L1/L2
- **Î•ÏÎ³Î±ÏƒÎ¯Î± Î“**: ÎœÎµÎ¯Ï‰ÏƒÎ· Î”Î¹Î±ÏƒÏ„Î¬ÏƒÎµÏ‰Î½ (PCA, CFS, Wrapper Methods)
- **Î•ÏÎ³Î±ÏƒÎ¯Î± Î”**: Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î¤Î¹Î¼ÏÎ½ Î³Î¹Î± Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿ 2025 ÎºÎ±Î¹ Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿ 2026

### English

This project implements a comprehensive machine learning pipeline for predicting Netflix (NFLX) stock prices using linear regression and advanced techniques. The project addresses four core tasks:

- **Task A**: Baseline Linear Regression with lagged features
- **Task B**: Polynomial Regression with L1/L2 Regularization
- **Task C**: Dimensionality Reduction (PCA, CFS, Wrapper Methods)
- **Task D**: Price Predictions for December 2025 and January 2026

---

## ğŸ¯ Î’Î±ÏƒÎ¹ÎºÎ¬ Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± / Key Results

### ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ ÎœÎ¿Î½Ï„Î­Î»Î¿ / Best Model

| ÎœÎµÏ„ÏÎ¹ÎºÎ® / Metric              | Î¤Î¹Î¼Î® / Value                   |
| ----------------------------- | ------------------------------ |
| **Î•Î¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· / Smoothing**    | sigma3 (Gaussian Ïƒ=3)          |
| **Î¥ÏƒÏ„ÎµÏÎ®ÏƒÎµÎ¹Ï‚ / Lags**         | 12 Î¼Î®Î½ÎµÏ‚ / 12 months           |
| **Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ / Features** | 24 (12 close + 12 volume lags) |
| **Training RMSE**             | $0.02                          |
| **Validation RMSE**           | $0.03                          |
| **Training RÂ²**               | 1.0000                         |
| **Validation RÂ²**             | 1.0000                         |

### Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÎœÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÏÎ½ Î¤Î¹Î¼ÏÎ½ / Future Price Predictions

| ÎœÎ®Î½Î±Ï‚ / Month                       | Î ÏÎ¿Î²Î»ÎµÏ†Î¸ÎµÎ¯ÏƒÎ± Î¤Î¹Î¼Î® / Predicted Price                |
| ----------------------------------- | -------------------------------------------------- |
| **Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿Ï‚ 2025 / December 2025** | **$1,175.48**                                      |
| **Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿Ï‚ 2026 / January 2026**  | **$1,175.95** (ÎºÎ±Ï„Î±ÏÏÎ±ÎºÏ„ÏÎ´Î·Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· / cascading) |

---

## ğŸ“ Î”Î¿Î¼Î® ÎˆÏÎ³Î¿Ï… / Project Structure

```
stock-price-linear-regression/
â”‚
â”œâ”€â”€ ğŸ“„ README_EL_EN.md                          # Î‘Ï…Ï„ÏŒ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ / This file
â”œâ”€â”€ ğŸ“„ README.md                                # Î‘Î³Î³Î»Î¹ÎºÏŒ README / English README
â”œâ”€â”€ ğŸ“„ ML_TERMINOLOGY_GLOSSARY_EL_EN.md         # Î“Î»Ï‰ÏƒÏƒÎ¬ÏÎ¹ ÏŒÏÏ‰Î½ / Terms glossary
â”œâ”€â”€ ğŸ“„ statistical_methods_of_ml.md             # Î ÎµÏÎ¹Î³ÏÎ±Ï†Î® ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ / Assignment description
â”œâ”€â”€ ğŸ“„ .env                                     # ÎšÎ»ÎµÎ¹Î´Î¯ API / API key
â”‚
â”œâ”€â”€ ğŸ“œ Scripts Python / Python Scripts:
â”‚   â”œâ”€â”€ step1_data_acquisition.py               # Î£Ï…Î»Î»Î¿Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ / Data acquisition
â”‚   â”œâ”€â”€ step2_feature_engineering.py            # ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ® Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ / Feature engineering
â”‚   â”œâ”€â”€ step3_baseline_linear_regression.py     # Î•ÏÎ³Î±ÏƒÎ¯Î± Î‘ / Task A
â”‚   â”œâ”€â”€ step4_polynomial_regression_regularization.py  # Î•ÏÎ³Î±ÏƒÎ¯Î± Î’ / Task B
â”‚   â”œâ”€â”€ step5_dimensionality_reduction.py       # Î•ÏÎ³Î±ÏƒÎ¯Î± Î“ / Task C
â”‚   â”œâ”€â”€ step6_future_predictions.py             # Î•ÏÎ³Î±ÏƒÎ¯Î± Î” (Î²Î±ÏƒÎ¹ÎºÎ®) / Task D (basic)
â”‚   â””â”€â”€ step6_future_predictions_improved.py    # Î•ÏÎ³Î±ÏƒÎ¯Î± Î” (Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î·) / Task D (improved)
â”‚
â”œâ”€â”€ ğŸ“‚ data/                                    # Î‘ÎºÎ±Ï„Î­ÏÎ³Î±ÏƒÏ„Î± & ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
â”‚   â”œâ”€â”€ nflx_monthly_raw.csv                   # Raw data
â”‚   â”œâ”€â”€ nflx_monthly_smoothed_sigma1.csv       # Î•Î¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· Ïƒ=1 / Smoothing Ïƒ=1
â”‚   â”œâ”€â”€ nflx_monthly_smoothed_sigma2.csv       # Î•Î¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· Ïƒ=2 / Smoothing Ïƒ=2
â”‚   â””â”€â”€ nflx_monthly_smoothed_sigma3.csv       # Î•Î¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· Ïƒ=3 / Smoothing Ïƒ=3
â”‚
â”œâ”€â”€ ğŸ“‚ features/                                # Î Î¯Î½Î±ÎºÎµÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ / Feature matrices
â”‚   â”œâ”€â”€ features_*.npz (16 ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ / 16 configurations)
â”‚   â”œâ”€â”€ scaler_*.pkl (Scalers)
â”‚   â””â”€â”€ metadata_*.csv (ÎœÎµÏ„Î±Î´ÎµÎ´Î¿Î¼Î­Î½Î± / Metadata)
â”‚
â”œâ”€â”€ ğŸ“‚ models/                                  # Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î± / Trained models
â”‚   â””â”€â”€ best_baseline_linear_regression.pkl
â”‚
â”œâ”€â”€ ğŸ“‚ results/                                 # Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± & Î±Ï€ÎµÎ¹ÎºÎ¿Î½Î¯ÏƒÎµÎ¹Ï‚ / Results & visualizations
â”‚   â”œâ”€â”€ ğŸ“Š Î“ÏÎ±Ï†Î®Î¼Î±Ï„Î± / Plots:
â”‚   â”‚   â”œâ”€â”€ comprehensive_predictions_comparison.png
â”‚   â”‚   â”œâ”€â”€ validation_rmse_comparison_improved.png
â”‚   â”‚   â”œâ”€â”€ best_model_forecast_with_history.png
â”‚   â”‚   â”œâ”€â”€ baseline_performance_by_config.png
â”‚   â”‚   â”œâ”€â”€ polynomial_regularization_paths.png
â”‚   â”‚   â””â”€â”€ dimensionality_reduction_comparison.png
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‹ Î Î¯Î½Î±ÎºÎµÏ‚ CSV / CSV Tables:
â”‚   â”‚   â”œâ”€â”€ all_models_predictions.csv
â”‚   â”‚   â”œâ”€â”€ baseline_linear_regression_results.csv
â”‚   â”‚   â”œâ”€â”€ polynomial_regression_comparison.csv
â”‚   â”‚   â””â”€â”€ dimensionality_reduction_results.csv
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“„ Î‘Î½Î±Ï†Î¿ÏÎ­Ï‚ / Reports:
â”‚       â”œâ”€â”€ FINAL_PREDICTIONS_REPORT_EL_EN.txt  # Î”Î¯Î³Î»Ï‰ÏƒÏƒÎ· Î±Î½Î±Ï†Î¿ÏÎ¬ / Bilingual report
â”‚       â””â”€â”€ FINAL_PROJECT_SUMMARY.txt           # Î‘Î³Î³Î»Î¹ÎºÎ® Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· / English summary
â”‚
â””â”€â”€ ğŸ“‚ Provided Code/                           # Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± ÎºÎ±Î¸Î·Î³Î·Ï„Î® / Teacher's examples
    â””â”€â”€ *.ipynb (11 notebooks)
```

---

## ğŸš€ Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· & Î•ÎºÏ„Î­Î»ÎµÏƒÎ· / Installation & Execution

### Î ÏÎ¿Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î± / Prerequisites

```bash
Python 3.8+
```

### Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î’Î¹Î²Î»Î¹Î¿Î¸Î·ÎºÏÎ½ / Install Libraries

```bash
pip install numpy pandas scikit-learn scipy matplotlib requests python-dateutil
```

### Î¡ÏÎ¸Î¼Î¹ÏƒÎ· API / API Setup

1. Î•Î³Î³ÏÎ±Ï†Î® ÏƒÏ„Î¿ Alpha Vantage: https://www.alphavantage.co/
   Sign up at Alpha Vantage: https://www.alphavantage.co/

2. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï… `.env`:
   Create `.env` file:

```
api_key=YOUR_API_KEY_HERE
```

### Î Î»Î®ÏÎ·Ï‚ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· / Full Execution

```bash
# Î’Î®Î¼Î± 1: Î£Ï…Î»Î»Î¿Î³Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ / Step 1: Data Acquisition
python step1_data_acquisition.py

# Î’Î®Î¼Î± 2: ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ® Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ / Step 2: Feature Engineering
python step2_feature_engineering.py

# Î’Î®Î¼Î± 3: Baseline Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· (Î•ÏÎ³Î±ÏƒÎ¯Î± Î‘) / Step 3: Baseline Regression (Task A)
python step3_baseline_linear_regression.py

# Î’Î®Î¼Î± 4: Î Î¿Î»Ï…Ï‰Î½Ï…Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· (Î•ÏÎ³Î±ÏƒÎ¯Î± Î’) / Step 4: Polynomial Regression (Task B)
python step4_polynomial_regression_regularization.py

# Î’Î®Î¼Î± 5: ÎœÎµÎ¯Ï‰ÏƒÎ· Î”Î¹Î±ÏƒÏ„Î¬ÏƒÎµÏ‰Î½ (Î•ÏÎ³Î±ÏƒÎ¯Î± Î“) / Step 5: Dimensionality Reduction (Task C)
python step5_dimensionality_reduction.py

# Î’Î®Î¼Î± 6: Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÎœÎ­Î»Î»Î¿Î½Ï„Î¿Ï‚ (Î•ÏÎ³Î±ÏƒÎ¯Î± Î”) / Step 6: Future Predictions (Task D)
# Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î­ÎºÎ´Î¿ÏƒÎ· - Î´Î¿ÎºÎ¹Î¼Î¬Î¶ÎµÎ¹ ÎŸÎ›Î‘ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± / Improved version - tests ALL models
python step6_future_predictions_improved.py
```

---

## ğŸ“Š ÎœÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¯Î± / Methodology

### 1. Î£Ï…Î»Î»Î¿Î³Î® & Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ / Data Acquisition & Preprocessing

#### Î•Î»Î»Î·Î½Î¹ÎºÎ¬

- **Î Î·Î³Î®**: Alpha Vantage API (TIME_SERIES_DAILY)
- **Î§ÏÎ¿Î½Î¹ÎºÏŒ Î•ÏÏÎ¿Ï‚**: ÎœÎ¬Î¹Î¿Ï‚ 2002 - ÎÎ¿Î­Î¼Î²ÏÎ¹Î¿Ï‚ 2025 (283 Î¼Î®Î½ÎµÏ‚)
- **Î£Ï…Î³ÎºÎ­Î½Ï„ÏÏ‰ÏƒÎ·**: Î—Î¼ÎµÏÎ®ÏƒÎ¹Î± â†’ ÎœÎ·Î½Î¹Î±Î¯Î± Î¼Î­ÏƒÎ± (close, volume)
- **Î•Î¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·**: Î¦Î¯Î»Ï„ÏÎ¿ Gauss Î¼Îµ Ïƒ âˆˆ {1, 2, 3}
- **Î‘Î¹Ï„Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·**: ÎœÎµÎ¹ÏÎ½ÎµÎ¹ Î¸ÏŒÏÏ…Î²Î¿ Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï„Î¬ÏƒÎµÎ¹Ï‚

#### English

- **Source**: Alpha Vantage API (TIME_SERIES_DAILY)
- **Time Range**: May 2002 - November 2025 (283 months)
- **Aggregation**: Daily â†’ Monthly averages (close, volume)
- **Smoothing**: Gaussian filter with Ïƒ âˆˆ {1, 2, 3}
- **Rationale**: Reduces noise while preserving trends

### 2. ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ® Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ / Feature Engineering

#### Î•Î»Î»Î·Î½Î¹ÎºÎ¬

- **Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î¥ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚**:
  - `close_t-1` Î­Ï‰Ï‚ `close_t-N`: Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ ÎºÎ»ÎµÎ¹ÏƒÎ¯Î¼Î±Ï„Î¿Ï‚
  - `volume_t-1` Î­Ï‰Ï‚ `volume_t-N`: Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿Î¹ ÏŒÎ³ÎºÎ¿Î¹ ÏƒÏ…Î½Î±Î»Î»Î±Î³ÏÎ½
- **Î”Î¿ÎºÎ¹Î¼Î±ÏƒÎ¼Î­Î½Î± Î Î±ÏÎ¬Î¸Ï…ÏÎ±**: N âˆˆ {3, 6, 9, 12} Î¼Î®Î½ÎµÏ‚
- **ÎšÎ»Î¹Î¼Î¬ÎºÏ‰ÏƒÎ·**: StandardScaler (z-score ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·)
- **Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½**:
  - Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·: Î ÏÎ¹Î½ Ï„Î¿ 2025 (260-269 Î´ÎµÎ¯Î³Î¼Î±Ï„Î±)
  - Î•Ï€Î¹ÎºÏÏÏ‰ÏƒÎ·: 2025 (11 Î´ÎµÎ¯Î³Î¼Î±Ï„Î±)
  - **ÎšÏÎ¯ÏƒÎ¹Î¼Î¿**: Î§ÏÎ¿Î½Î¿Î»Î¿Î³Î¹ÎºÏŒÏ‚ Î´Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ (Ï‡Ï‰ÏÎ¯Ï‚ Î±Î½Î±ÎºÎ¬Ï„ÎµÎ¼Î±)

#### English

- **Lagged Features**:
  - `close_t-1` through `close_t-N`: Past closing prices
  - `volume_t-1` through `volume_t-N`: Past trading volumes
- **Tested Windows**: N âˆˆ {3, 6, 9, 12} months
- **Scaling**: StandardScaler (z-score normalization)
- **Data Split**:
  - Training: Pre-2025 (260-269 samples)
  - Validation: 2025 (11 samples)
  - **Critical**: Chronological split (no shuffling)

### 3. Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· & Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ / Model Training & Evaluation

#### Î•ÏÎ³Î±ÏƒÎ¯Î± Î‘: Baseline Î“ÏÎ±Î¼Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· / Task A: Baseline Linear Regression

**Î•Î»Î»Î·Î½Î¹ÎºÎ¬:**

- Î”Î¿ÎºÎ¹Î¼Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚: 16 (4 ÎµÏ€Î¯Ï€ÎµÎ´Î± ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚ Ã— 4 Ï€Î±ÏÎ¬Î¸Ï…ÏÎ± Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚)
- ÎœÎ¿Î½Ï„Î­Î»Î¿: Î‘Ï€Î»Î® Î“ÏÎ±Î¼Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· (OLS)
- ÎœÎµÏ„ÏÎ¹ÎºÎ­Ï‚: RMSE, MAE, RÂ²
- ÎšÎ±Î»ÏÏ„ÎµÏÎ·: sigma3, 12 lags â†’ RMSE $0.03, RÂ² 1.0000

**English:**

- Configurations Tested: 16 (4 smoothing levels Ã— 4 lag windows)
- Model: Ordinary Least Squares (OLS) Linear Regression
- Metrics: RMSE, MAE, RÂ²
- Best: sigma3, 12 lags â†’ RMSE $0.03, RÂ² 1.0000

#### Î•ÏÎ³Î±ÏƒÎ¯Î± Î’: Î Î¿Î»Ï…Ï‰Î½Ï…Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· / Task B: Polynomial Regression

**Î•Î»Î»Î·Î½Î¹ÎºÎ¬:**

- Î’Î±Î¸Î¼ÏŒÏ‚: 2 (24 Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ â†’ 325 Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬)
- **Ridge (L2)**:
  - ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ Î±: 0.1
  - Val RMSE: $8.98
  - ÎŒÎ»Î± Ï„Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î´Î¹Î±Ï„Î·ÏÎ¿ÏÎ½Ï„Î±Î¹
- **Lasso (L1)**:
  - ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ Î±: 0.001
  - Val RMSE: $9.47
  - 263/325 Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ ÎµÏ€Î¹Î»Î­Ï‡Î¸Î·ÎºÎ±Î½ (19.1% Î±ÏÎ±Î¯Ï‰ÏƒÎ·)
- Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î¤Î¿ baseline Ï…Ï€ÎµÏÏ„ÎµÏÎµÎ¯ Î»ÏŒÎ³Ï‰ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÎ®Ï‚ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚

**English:**

- Degree: 2 (24 features â†’ 325 features)
- **Ridge (L2)**:
  - Best Î±: 0.1
  - Val RMSE: $8.98
  - All features retained
- **Lasso (L1)**:
  - Best Î±: 0.001
  - Val RMSE: $9.47
  - 263/325 features selected (19.1% sparsity)
- Conclusion: Baseline outperforms due to effective smoothing

#### Î•ÏÎ³Î±ÏƒÎ¯Î± Î“: ÎœÎµÎ¯Ï‰ÏƒÎ· Î”Î¹Î±ÏƒÏ„Î¬ÏƒÎµÏ‰Î½ / Task C: Dimensionality Reduction

| ÎœÎ­Î¸Î¿Î´Î¿Ï‚ / Method                    | Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ / Features            | Val RMSE | Val RÂ² |
| ----------------------------------- | ------------------------------------ | -------- | ------ |
| **Baseline**                        | 24 (Ï€Î»Î®ÏÎ· / full)                    | $0.03    | 1.0000 |
| **PCA (95% Î´Î¹Î±ÎºÏÎ¼Î±Î½ÏƒÎ· / variance)** | 3 ÏƒÏ…Î½Î¹ÏƒÏ„ÏÏƒÎµÏ‚ / components            | $131.07  | -1.17  |
| **CFS**                             | 1 (close_t-1)                        | $21.91   | 0.9392 |
| **Sequential Forward Selection**    | 12 (ÏŒÎ»Î± close lags / all close lags) | $0.03    | 1.0000 |

**Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î± / Conclusion**: ÎŸÎ¹ Ï…ÏƒÏ„ÎµÏÎ®ÏƒÎµÎ¹Ï‚ close ÎµÏ€Î±ÏÎºÎ¿ÏÎ½Â· Î¿ ÏŒÎ³ÎºÎ¿Ï‚ Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹ ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î· Î±Î¾Î¯Î± / Close lags sufficient; volume adds minimal value

#### Î•ÏÎ³Î±ÏƒÎ¯Î± Î”: Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÎœÎ­Î»Î»Î¿Î½Ï„Î¿Ï‚ / Task D: Future Predictions

**Î•Î»Î»Î·Î½Î¹ÎºÎ¬:**

- **ÎœÎ­Î¸Î¿Î´Î¿Ï‚**: ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ baseline Î¼Î¿Î½Ï„Î­Î»Î¿ (sigma3, 12 lags)
- **Î”ÎµÎºÎ­Î¼Î²ÏÎ¹Î¿Ï‚ 2025**: $1,175.48 (Î¬Î¼ÎµÏƒÎ· Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·)
- **Î™Î±Î½Î¿Ï…Î¬ÏÎ¹Î¿Ï‚ 2026**: $1,175.95 (ÎºÎ±Ï„Î±ÏÏÎ±ÎºÏ„ÏÎ´Î·Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·)
- **ÎšÎ±Ï„Î±ÏÏÎ±ÎºÏ„ÏÎ´Î·Ï‚ Î ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ·**: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… Ï‰Ï‚ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏŒ Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ·Ï‚
- **Î£Î·Î¼ÎµÎ¯Ï‰ÏƒÎ·**: ÎœÎµÎ¹Ï‰Î¼Î­Î½Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Î»ÏŒÎ³Ï‰ Ï€Î¿Î»Î»Î±Ï€Î»Î±ÏƒÎ¹Î±ÏƒÎ¼Î¿Ï ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½

**English:**

- **Method**: Best baseline model (sigma3, 12 lags)
- **December 2025**: $1,175.48 (direct prediction)
- **January 2026**: $1,175.95 (cascading prediction)
- **Cascading Approach**: Uses December prediction as lag feature
- **Note**: Reduced accuracy due to error compounding

---

## ğŸ“ˆ Î’Î±ÏƒÎ¹ÎºÎ¬ Î•Ï…ÏÎ®Î¼Î±Ï„Î± / Key Findings

### Î•Î»Î»Î·Î½Î¹ÎºÎ¬

1. **Î— Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎµÎ¯Î½Î±Î¹ ÎšÏÎ¯ÏƒÎ¹Î¼Î·**: Î— Î²Î±ÏÎ¹Î¬ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· Gauss (Ïƒ=3) Î®Ï„Î±Î½ Î¿ Ï€Î¹Î¿ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒÏ‚ Ï€Î±ÏÎ¬Î³Î¿Î½Ï„Î±Ï‚ ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î±Ï‚, Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Î½Ï„Î±Ï‚ Î¸Î¿ÏÏ…Î²ÏÎ´Î· Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÎµ Ï€Î¿Î»Ï Ï€ÏÎ¿Î²Î»Î­ÏˆÎ¹Î¼Î± Î¼Î¿Ï„Î¯Î²Î±.

2. **Î•Ï€Î±ÏÎºÎ® Ï„Î± Î“ÏÎ±Î¼Î¼Î¹ÎºÎ¬ ÎœÎ¿Î½Ï„Î­Î»Î±**: ÎœÎµ ÏƒÏ‰ÏƒÏ„Î® Ï€ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±, Î· Î±Ï€Î»Î® Î³ÏÎ±Î¼Î¼Î¹ÎºÎ® Ï€Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· Ï€Î­Ï„Ï…Ï‡Îµ ÏƒÏ‡ÎµÎ´ÏŒÎ½ Ï„Î­Î»ÎµÎ¹Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±. Î¤Î± ÏƒÏÎ½Î¸ÎµÏ„Î± Ï€Î¿Î»Ï…Ï‰Î½Ï…Î¼Î¹ÎºÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î®Ï„Î±Î½ Ï€ÎµÏÎ¹Ï„Ï„Î¬.

3. **Î’Î­Î»Ï„Î¹ÏƒÏ„Î¿ Î Î±ÏÎ¬Î¸Ï…ÏÎ¿ Î‘Î½Î±Î´ÏÎ¿Î¼Î®Ï‚**: Î¤Î¿ Ï€Î±ÏÎ¬Î¸Ï…ÏÎ¿ 12 Î¼Î·Î½ÏÎ½ ÏƒÏ…Î½Î­Î»Î±Î²Îµ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÎ¬ Ï„ÏŒÏƒÎ¿ Ï„Î·Î½ Î²ÏÎ±Ï‡Ï…Ï€ÏÏŒÎ¸ÎµÏƒÎ¼Î· Î¿ÏÎ¼Î® ÏŒÏƒÎ¿ ÎºÎ±Î¹ Ï„Î¹Ï‚ Î¼Î±ÎºÏÎ¿Ï€ÏÏŒÎ¸ÎµÏƒÎ¼ÎµÏ‚ Ï„Î¬ÏƒÎµÎ¹Ï‚.

4. **Î£Î·Î¼Î±ÏƒÎ¯Î± Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½**: ÎŸÎ¹ Ï…ÏƒÏ„ÎµÏÎ®ÏƒÎµÎ¹Ï‚ Ï„Î¹Î¼ÏÎ½ ÎºÎ»ÎµÎ¹ÏƒÎ¯Î¼Î±Ï„Î¿Ï‚ Ï€Î¿Î»Ï Ï€Î¹Î¿ ÎµÎ½Î·Î¼ÎµÏÏ‰Ï„Î¹ÎºÎ­Ï‚ Î±Ï€ÏŒ Ï„Î¿Î½ ÏŒÎ³ÎºÎ¿. Î— Sequential Forward Selection Ï„Î¿ ÎµÏ€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎµ ÎµÏ€Î¹Î»Î­Î³Î¿Î½Ï„Î±Ï‚ Î¼ÏŒÎ½Î¿ Ï…ÏƒÏ„ÎµÏÎ®ÏƒÎµÎ¹Ï‚ close.

5. **Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯ PCA**: Î— PCA Î±Ï€Î­Ï„Ï…Ï‡Îµ ÏƒÎµ Î²Î±ÏÎ¹Î¬ ÎµÎ¾Î¿Î¼Î±Î»Ï…Î¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÎµÏ€ÎµÎ¹Î´Î®:
   - Î— ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· Î®Î´Î· Î¼ÎµÎ¯Ï‰ÏƒÎµ ÎµÎ½Î½Î¿Î¹Î¿Î»Î¿Î³Î¹ÎºÎ¬ Ï„Î¹Ï‚ Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚
   - ÎŸ Î³ÏÎ±Î¼Î¼Î¹ÎºÏŒÏ‚ Î¼ÎµÏ„Î±ÏƒÏ‡Î·Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚ Î´ÎµÎ½ Î¼Ï€Î¿ÏÎ¿ÏÏƒÎµ Î½Î± Î²ÎµÎ»Ï„Î¹ÏÏƒÎµÎ¹ Ï„Î± ÎµÎ¾Î¿Î¼Î±Î»Ï…Î¼Î­Î½Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬
   - ÎšÏÎ¯ÏƒÎ¹Î¼ÎµÏ‚ Ï‡ÏÎ¿Î½Î¹ÎºÎ­Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Ï‡Î¬Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î¿Î½ Î¼ÎµÏ„Î±ÏƒÏ‡Î·Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ

### English

1. **Preprocessing is Critical**: Heavy Gaussian smoothing (Ïƒ=3) was the most important success factor, transforming noisy data into highly predictable patterns.

2. **Linear Models Sufficient**: With proper preprocessing, simple linear regression achieved near-perfect results. Complex polynomial features were unnecessary.

3. **Optimal Lookback Window**: The 12-month lag window effectively captured both short-term momentum and long-term trends.

4. **Feature Importance**: Close price lags far more informative than volume. Sequential Forward Selection confirmed this by selecting only close lags.

5. **PCA Limitations**: PCA failed on heavily smoothed data because:
   - Smoothing already reduced dimensionality conceptually
   - Linear transformation couldn't improve on smoothed features
   - Critical temporal information was lost in transformation

---

## ğŸ“Š Î‘Ï€ÎµÎ¹ÎºÎ¿Î½Î¯ÏƒÎµÎ¹Ï‚ / Visualizations

### Î’Î±ÏƒÎ¹ÎºÎ¬ Î“ÏÎ±Ï†Î®Î¼Î±Ï„Î± / Main Plots

1. **comprehensive_predictions_comparison.png**

   - Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ 16 Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ / All 16 models comparison
   - Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï… & Î™Î±Î½Î¿Ï…Î±ÏÎ¯Î¿Ï… / December & January predictions
   - RMSE vs Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ / RMSE vs predictions
   - Heatmap Ï„Î¹Î¼ÏÎ½ / Price heatmap

2. **validation_rmse_comparison_improved.png**

   - Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· RMSE / Improved RMSE visualization
   - ÎŒÎ»ÎµÏ‚ Î¿Î¹ ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î¼Îµ Ï‡ÏÏÎ¼Î±Ï„Î± / All configurations color-coded
   - Î•Ï€Î¹ÏƒÎ®Î¼Î±Î½ÏƒÎ· ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… / Best model highlighted
   - **Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ·**: Î£Ï‰ÏƒÏ„Î­Ï‚ ÏƒÏ…Î½Î´Î­ÏƒÎµÎ¹Ï‚ Î³ÏÎ±Î¼Î¼ÏÎ½ / **Fixed**: Correct line connections

3. **best_model_forecast_with_history.png**
   - Î™ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± + Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ / Historical data + predictions
   - Î‘Î½Î½Î¿Ï„Î¬Ï„Îµ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ / Annotated predictions
   - Î Î»Î®ÏÎ·Ï‚ Ï‡ÏÎ¿Î½Î¿ÏƒÎµÎ¹ÏÎ¬ / Full time series

---

## ğŸ“ Î¤ÎµÏ‡Î½Î¹ÎºÎ­Ï‚ Î›ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚ / Technical Details

### Î Î¿Î»Ï…Ï€Î»Î¿ÎºÏŒÏ„Î·Ï„Î± Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏÎ½ / Computational Complexity

- **Î£Ï…Î»Î»Î¿Î³Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ / Data Acquisition**: O(n) ÎºÎ»Î®ÏƒÎµÎ¹Ï‚ API + ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± / O(n) API calls + processing
- **ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ® Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ / Feature Engineering**: O(n Ã— m) ÏŒÏ€Î¿Ï… n=Î´ÎµÎ¯Î³Î¼Î±Ï„Î±, m=Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ / where n=samples, m=features
- **Î“ÏÎ±Î¼Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· / Linear Regression**: O(mÂ² Ã— n) Î³Î¹Î± Î»ÏÏƒÎ· OLS / for OLS solution
- **Î Î¿Î»Ï…Ï‰Î½Ï…Î¼Î¹ÎºÎ® (Î²Î±Î¸Î¼ÏŒÏ‚ 2) / Polynomial (degree 2)**: O(mâ´ Ã— n)
- **Sequential Forward Selection**: O(mÂ² Ã— k) ÎµÎºÏ€Î±Î¹Î´ÎµÏÏƒÎµÎ¹Ï‚ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ / model trainings

### Î‘Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ ÎœÎ½Î®Î¼Î·Ï‚ / Memory Requirements

- **Î‘ÎºÎ±Ï„Î­ÏÎ³Î±ÏƒÏ„Î± Î”ÎµÎ´Î¿Î¼Î­Î½Î± / Raw Data**: ~5,000 Î·Î¼ÎµÏÎ®ÏƒÎ¹Î± Î±ÏÏ‡ÎµÎ¯Î± â†’ ~1 MB / ~5,000 daily records â†’ ~1 MB
- **Î Î¯Î½Î±ÎºÎµÏ‚ Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ / Feature Matrices**: 16 ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Ã— 2 ÏƒÏÎ½Î¿Î»Î± â†’ ~10 MB / 16 configurations Ã— 2 sets â†’ ~10 MB
- **ÎœÎ¿Î½Ï„Î­Î»Î± / Models**: < 1 MB ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¬ / < 1 MB total

### Î§ÏÏŒÎ½Î¿Ï‚ Î•ÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚ / Runtime (Î ÏÎ¿ÏƒÎµÎ³Î³Î¹ÏƒÏ„Î¹ÎºÏŒÏ‚ / Approximate)

- Î’Î®Î¼Î± 1 / Step 1: 30-60 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î± / seconds (ÎºÎ»Î®ÏƒÎ· API / API call)
- Î’Î®Î¼Î± 2 / Step 2: 5-10 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î± / seconds
- Î’Î®Î¼Î± 3 / Step 3: 2-3 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î± / seconds
- Î’Î®Î¼Î± 4 / Step 4: 5-10 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î± / seconds
- Î’Î®Î¼Î± 5 / Step 5: 30-60 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î± / seconds (Sequential Selection)
- Î’Î®Î¼Î± 6 (Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î¿) / Step 6 (improved): 10-15 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î± / seconds

**Î£Ï…Î½Î¿Î»Î¹ÎºÏŒÏ‚ Î§ÏÏŒÎ½Î¿Ï‚ / Total Runtime**: ~2-3 Î»ÎµÏ€Ï„Î¬ / minutes

---

## âš ï¸ Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯ / Limitations

### Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯ ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… / Model Limitations

#### Î•Î»Î»Î·Î½Î¹ÎºÎ¬

1. **Î‘Î½Ï„Î±Î»Î»Î±Î³Î® Î’Î±ÏÎ¹Î¬Ï‚ Î•Î¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚**: ÎœÏ€Î¿ÏÎµÎ¯ Î½Î± ÎºÎ±Î¸Ï…ÏƒÏ„ÎµÏÎ®ÏƒÎµÎ¹ Ï„Î·Î½ Î±Î½Ï„Î¯Î´ÏÎ±ÏƒÎ· ÏƒÎµ Î¾Î±Ï†Î½Î¹ÎºÎ­Ï‚ Î±Î»Î»Î±Î³Î­Ï‚ Î±Î³Î¿ÏÎ¬Ï‚
2. **Î“ÏÎ±Î¼Î¼Î¹ÎºÎ® Î¥Ï€ÏŒÎ¸ÎµÏƒÎ·**: Î¥Ï€Î¿Î¸Î­Ï„ÎµÎ¹ ÏŒÏ„Î¹ Ï„Î± Ï€Î±ÏÎµÎ»Î¸Î¿Î½Ï„Î¹ÎºÎ¬ Î¼Î¿Ï„Î¯Î²Î± ÏƒÏ…Î½ÎµÏ‡Î¯Î¶Î¿Î½Ï„Î±Î¹
3. **Î•Î¾Ï‰Ï„ÎµÏÎ¹ÎºÎ¬ Î“ÎµÎ³Î¿Î½ÏŒÏ„Î±**: Î”ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÏƒÏ…Î»Î»Î¬Î²ÎµÎ¹ Î±Î½Î±ÎºÎ¿Î¹Î½ÏÏƒÎµÎ¹Ï‚ ÎºÎµÏÎ´ÏÎ½, ÎºÏÎ±Ï‡ Î±Î³Î¿ÏÎ¬Ï‚, ÎµÎ¹Î´Î®ÏƒÎµÎ¹Ï‚
4. **Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î± Î”ÎµÎ´Î¿Î¼Î­Î½Î± Î•Ï€Î¹ÎºÏÏÏ‰ÏƒÎ·Ï‚**: ÎœÏŒÎ½Î¿ 11 Î¼Î®Î½ÎµÏ‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ 2025
5. **ÎšÎ±Ï„Î±ÏÏÎ±ÎºÏ„ÏÎ´Î·Ï‚ Î ÏÏŒÎ²Î»ÎµÏˆÎ·**: Î— Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î™Î±Î½Î¿Ï…Î±ÏÎ¯Î¿Ï… Î­Ï‡ÎµÎ¹ Î±Î²ÎµÎ²Î±Î¹ÏŒÏ„Î·Ï„Î± Î»ÏŒÎ³Ï‰ Ï‡ÏÎ®ÏƒÎ·Ï‚ Ï€ÏÎ¿Î²Î»ÎµÏ†Î¸Î­Î½Ï„Î¿Ï‚ Î”ÎµÎºÎµÎ¼Î²ÏÎ¯Î¿Ï…

#### English

1. **Heavy Smoothing Trade-off**: May delay reaction to sudden market changes
2. **Linear Assumption**: Assumes past patterns continue
3. **External Events**: Cannot capture earnings reports, market crashes, news
4. **Limited Validation Data**: Only 11 months of 2025 data
5. **Cascading Prediction**: January prediction has uncertainty due to using predicted December

---

## ğŸ’¡ Î£Ï…ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ / Recommendations

### Î“Î¹Î± Î Î±ÏÎ±Î³Ï‰Î³Î¹ÎºÎ® Î§ÏÎ®ÏƒÎ· / For Production Use

#### Î•Î»Î»Î·Î½Î¹ÎºÎ¬

1. **Î£Ï…Ï‡Î½ÏŒÏ„Î·Ï„Î± Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ·Ï‚**: Î•Ï€Î±Î½ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î¼Î·Î½Î¹Î±Î¯Î± Î¼Îµ Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
2. **Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ·**: Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· ÏƒÏ†Î±Î»Î¼Î¬Ï„Ï‰Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚ Î³Î¹Î± Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î±Î»Î»Î±Î³ÏÎ½ ÎºÎ±Î¸ÎµÏƒÏ„ÏÏ„Î¿Ï‚
3. **Ensemble**: Î£Ï…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ Î±Ï€ÏŒ Ï€Î¿Î»Î»Î±Ï€Î»Î­Ï‚ Ï„Î¹Î¼Î­Ï‚ Ïƒ
4. **Î”Î¹Î±ÏƒÏ„Î®Î¼Î±Ï„Î± Î•Î¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î·Ï‚**: Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· bootstrap Î® Bayesian Î¼ÎµÎ¸ÏŒÎ´Ï‰Î½
5. **Î•Ï€Î­ÎºÏ„Î±ÏƒÎ· Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½**: Î£Ï…Î¼Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· Î´ÎµÎ¹ÎºÏ„ÏÎ½ Î±Î³Î¿ÏÎ¬Ï‚ (S&P 500), Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚ ÎºÎ»Î¬Î´Î¿Ï…

#### English

1. **Update Frequency**: Retrain model monthly with new data
2. **Monitoring**: Track prediction errors to detect regime changes
3. **Ensemble**: Combine predictions from multiple Ïƒ values
4. **Confidence Intervals**: Add bootstrap or Bayesian methods
5. **Feature Expansion**: Include market indices (S&P 500), sector performance

---

## ğŸ“š Î‘Î½Î±Ï†Î¿ÏÎ­Ï‚ / References

### Î‘Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ Î•ÏÎ³Î±ÏƒÎ¯Î±Ï‚ / Assignment Requirements

- **ÎœÎ¬Î¸Î·Î¼Î± / Course**: Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ ÎœÎ­Î¸Î¿Î´Î¿Î¹ ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ®Ï‚ ÎœÎ¬Î¸Î·ÏƒÎ·Ï‚ / Statistical Methods of Machine Learning
- **Î•ÏÎ³Î±ÏƒÎ¯Î± / Task**: Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î¤Î¹Î¼ÏÎ½ ÎœÎµÏ„Î¿Ï‡ÏÎ½ Î¼Îµ Î“ÏÎ±Î¼Î¼Î¹ÎºÎ® Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ· / Predicting Stock Prices with Linear Regression
- **Î£ÏÎ¼Î²Î¿Î»Î¿ / Symbol**: NFLX (Netflix, Inc.)
- **API**: Alpha Vantage (https://www.alphavantage.co/)

### Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Î’Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚ / Key Libraries

- **scikit-learn**: ÎœÎ¿Î½Ï„Î­Î»Î± & Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚ Î¼Î·Ï‡Î±Î½Î¹ÎºÎ®Ï‚ Î¼Î¬Î¸Î·ÏƒÎ·Ï‚ / Machine learning models & metrics
- **pandas**: Î§ÎµÎ¹ÏÎ¹ÏƒÎ¼ÏŒÏ‚ & Î±Î½Î¬Î»Ï…ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ / Data manipulation & analysis
- **numpy**: Î‘ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¿Î¯ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿Î¯ / Numerical computing
- **scipy**: Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Gauss / Gaussian filtering (scipy.ndimage.gaussian_filter1d)
- **matplotlib**: ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· / Visualization

---

## ğŸ“ Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· / Support

### Î“Î¹Î± Î•ÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ / For Questions

1. Î•Î»Î­Î³Î¾Ï„Îµ Ï„Î¿ / Check `FINAL_PREDICTIONS_REPORT_EL_EN.txt` Î³Î¹Î± Î»ÎµÏ€Ï„Î¿Î¼ÎµÏÎ® Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± / for detailed results
2. Î‘Î½Î±Ï„ÏÎ­Î¾Ï„Îµ ÏƒÏ„Î¿ Î³Î»Ï‰ÏƒÏƒÎ¬ÏÎ¹ / Consult glossary: `ML_TERMINOLOGY_GLOSSARY_EL_EN.md`
3. Î”Î¹Î±Î²Î¬ÏƒÏ„Îµ ÏƒÏ‡ÏŒÎ»Î¹Î± ÎºÏÎ´Î¹ÎºÎ± / Review inline code comments (ÎµÎºÏ„ÎµÎ½Î®Ï‚ Ï„ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· / extensive documentation)

---

## ğŸ“„ Î†Î´ÎµÎ¹Î± / License

Î‘Ï…Ï„ÏŒ Ï„Î¿ Î­ÏÎ³Î¿ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ Î³Î¹Î± Î±ÎºÎ±Î´Î·Î¼Î±ÏŠÎºÎ¿ÏÏ‚ ÏƒÎºÎ¿Ï€Î¿ÏÏ‚ Ï‰Ï‚ Î¼Î­ÏÎ¿Ï‚ Î¼Î¹Î±Ï‚ ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ Î¼Î±Î¸Î®Î¼Î±Ï„Î¿Ï‚ ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ®Ï‚ ÎœÎ¬Î¸Î·ÏƒÎ·Ï‚.

This project was created for academic purposes as part of a Machine Learning course assignment.

---

**Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î± Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· / Last Updated**: 17 ÎÎ¿ÎµÎ¼Î²ÏÎ¯Î¿Ï… 2025 / November 17, 2025  
**ÎšÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· ÎˆÏÎ³Î¿Ï… / Project Status**: ÎŸÎ»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î¿ âœ“ / Complete âœ“

ÎŒÎ»ÎµÏ‚ Î¿Î¹ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚ (Î‘, Î’, Î“, Î”) Ï…Î»Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎ±Î½ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚ Î¼Îµ ÎµÎºÏ„ÎµÎ½Î® Ï„ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ·.  
All tasks (A, B, C, D) successfully implemented with extensive documentation.
